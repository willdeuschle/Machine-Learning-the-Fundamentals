\chapter{Mixture Models}
TODO better intro for mixture models.
In this chapter we will be exploring what are known as mixture models. The idea behind mixture models is quite intuitive: the real world produces data resulting from a combination of different forces. It should then be possible to model these forces individually and determine how they mix together to form the individual units of data that we observe. We will be diving into the details of the mathematic and statistical ideas underpinning mixture models, as well as how they can be used.

\section{Motivation}
Mixture models are very useful for handling data that involves \textit{latent variables}.

\begin{definition}{Latent Variable}{latent-variable}
    A latent variable is a piece of data that is not observed, but that influences the observed data. Nonetheless, we often create models that try to capture the behavior of our latent variables.
\end{definition}

Very often, we are unable to observe all the data present in a given system. For example, if we are measuring the snout length of different animals, but we only get to see the snout measurements, the latent variables would be the type of animal we are measuring. For most real examples of data generation, we will only have access to a portion of the data and the rest will be hidden from us. However, if we can find some way to also model the latent variables, our model will potentially be much more powerful and effective.

Let's consider what the directed graphical model involving a latent variable might look like:

TODO image here of a latent variable model.

One common means of modeling a latent variable model, and the topic of this chapter, is known as a \textit{mixture model}.

\begin{definition}{Mixture Model}{mixture-model}
    A mixture model captures the behavior of data coming from a combination of different distributions.
\end{definition}

At a high level, a mixture model operates under the assumption that our data is generated by first sampling a discrete class, and then sampling a data point from within that category according to the distribution for that category. For the example of animal snouts, we would first sample a species of animal, and then based on the distribution of snout lengths in that species, we would sample an observation to get a complete data point.

Probabilistically, sampling a class (which is our latent variable, since we don't actually observe this) happens according to a Categorical distribution, and we typically refer to the latent variable as $z$. Thus:
\begin{align*}
    p(z = C_{k} ; \boldsymbol{\theta}) = \theta_{k}
\end{align*}
where $C_{k}$ is class $k$, and $\boldsymbol{\theta}$ is the parameter to the Categorical distribution that specifies the probability of drawing each class. Then, once we have a class, we have a conditional distribution for the actual observed data point:
\begin{align*}
    p(\textbf{x} | z = C_{k}; \textbf{w})
\end{align*}
This distribution depends on the type of data we are observing, and is parameterized by an arbitrary parameter $\textbf{w}$ whose form depends on the distribution chosen to represent the class-conditional distribution. For the case of snout lengths, and many other examples, this conditional distribution will often be Gaussian, in which case our model is known as a \textbf{Gaussian Mixture Model}. We will discuss Gaussian Mixture Models in more detail later in the chapter.

If we can effectively model the distribution of our observed data points and the latent variables that helped produced the data, we will be able to ask interesting questions of our model. For example, upon observing a new data point $\textbf{x}'$ we will be able to produce a probability that it came from a specific class $z' = C_k$ using Bayes' rule and our model parameters:
\begin{align*}
    p(z' = C_k | \textbf{x}') = \frac{p(\textbf{x}' | z' = C_{k}; \textbf{w})p(z' = C_{k} ; \boldsymbol{\theta})}{\sum_{k'} p(\textbf{x}' | z' = C_{k'}; \textbf{w})p(z' = C_{k'} ; \boldsymbol{\theta})}
\end{align*}
Furthermore, after modeling the generative process, we will be able to generate new data points by sampling from our categorical class distribution, and then from the class-conditional distribution for that category:
\begin{align*}
    z \sim Cat(\boldsymbol{\theta}) \\
    \textbf{x} \sim p(\textbf{x} | z = C_{k}; \textbf{w})
\end{align*}
Finally, it will also be possible for us to get a sense of how many classes our data falls into, if that was not something we were aware of a priori.

\begin{mlcube}{Mixture Models}
While the class of data in a mixture model will typically be discrete, the class-conditional distribution ($p(x | z)$) can be either discrete or continuous, depending on the form of the data that we observe. Notice also that this is an unsupervised technique: while we have a data set $\textbf{X}$ of observations, our goal is not to make predictions. Rather, we are trying to model the generative process of this data by accounting for the latent variables that helped generate it. Finally, this is a probabilistic model both for the latent variables and for our observed data.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous/Discrete & Unsupervised & Yes \\
    \end{tabular}
\end{center}
\end{mlcube}

\section{Applications}
Since most of the data we observe in our world has some sort of unobserved category-based phenomenon associated with it, there are a wide variety of applications for mixture models. We present a few here:
\begin{enumerate}
    \item Handwriting image recognition. The categories are given by the characters (letters, numbers, etc.) and the class-conditional is a distribution over what each of those characters might look like.
    \item Modeling the topics in a document. Words or sentences in a document can be thought of as corresponding to specific topics and the class-condition for a specific topic determines a distribution over the words you might see for that topic.
    \item Vehicle prices. Vehicle can be categorized a number of different ways (gas mileage, size, safety, etc.). The class-conditional for a given category determines the distribution over the price of that vehicle.
\end{enumerate}

\section{Fitting a Model}
We've now defined a general form for a mixture model: we have a distribution $p(z; \theta)$ over our classes and a distribution $p(x|z; w)$ over our class-conditional distribution. A natural approach would be to compute the maximum likelihood values for our parameters $\theta$ and $w$. Let's consider how me might go about this for a mixture model.

\subsection{Maximum Likelihood for Mixture Models}
Ultimately, we would like to maximize the likelihood of our observed data as normal. Because we don't actually observe the latent variables $z$ which determine the class of each observed data point, we can simply sum over the possible classes for each of our $N$ data points as follows:
\begin{align*}
    p(\textbf{X}; w) = \prod_{n=1}^{N} \sum_{c=0}^{C} p(x_{n}, z_{n, c}; \theta, w)
\end{align*}
Taking the logarithm to get our log-likelihood as usual:
\begin{align} \label{intractable-log-likelihood}
    \log p(\textbf{X}; w) = \sum_{n=1}^{N} \log \sum_{c=0}^{C} p(x_{n}, z_{n, c}; \theta, w)
\end{align}
It may not be immediately obvious, but under this setup, the maximum likelihood calculation for our parameters $\theta, w$ is now intractable. The summation over the $C$ classes of our latent variable $z_{n}$, which is required because we don't actually observe those classes, is inside of the logarithm, which prevents us from arriving at a closed form solution (it may be helpful to try to solve it yourself, you'll realize that consolidating a summation inside of a logarithm is not possible). The rest of this chapter will deal with how we can optimize our mixture model in the face of this challenge.

\subsection{Complete Data Log Likelihood}
We have a problem with computing the MLE for our model parameters. If we only knew which classes our data points came from, we would be able to calculate $\log p(z, x)$ with relative ease because we would no longer require a summation in the log:
\begin{align} \label{complete-data-log-likelihood}
    \log p(\textbf{X}, \textbf{Z}) &= \sum_{n=1}^{N} \log p(x_n, z_n; \theta, w) \\
    &= \sum_{n=1}^{N} \log[p(x_n | z_n = c; w) p(z_n = c; \theta)] \\
    &= \sum_{n=1}^{N} \log p(x_n | z_n = c; w) + \log p(z_n = c; \theta) \\
\end{align}
Notice that because we've now observed $z_{n}$, we don't have to marginalize over its possible values. This motivates an interesting approach that takes advantage of our ability to work with $p(z, x)$ if we only knew $z$.

The expression p(z, x) is known as the \textit{complete data} because it assumes that we have both our observation $x$ and the classs $z$ that $x$ came from. Our ability to efficiently calculate the complete data log likelihood $\log p(z, x)$ is the crucial piece of the algorithm we will present to optimize our mixture model parameters. This algorithm is known as Expectation-Maximization, or EM.

\section{Expectation-Maximization (EM)}
\textbf{TODO: explain at a high level why we're using EM, how it works. Make connection to k-means clustering here? maybe just add its own section to discuss this connection}

The motivation for the EM algorithm, as presented in the previous section, is that we do not have a closed form optimization for our model parameters because of a summation inside of a logarithm. This summation was required because we didn't observe a crucial piece of data, the class $z_{n}$, and therefore we had to sum over its values.

EM uses an iterative approach to optimize our model parameters. It proposes a value for $z_{n}$ using an expectation calculation, and then based on that proposed value, it maximizes our complete-data log likelihood with respect to the model parameters $\boldsymbol{\theta}$ and $\textbf{w}$ via a standard MLE procedure.

Notice that EM is composed of two distinct steps: taking an expectation over our class parameters and performing a maximization over our model parameters. These two steps (expectation and maximization) obviously give the algorithm its name, but more generally, this type of approach is also referred to as \textit{coordinate ascent}. The high level idea behind coordinate ascent is that we can replace a hard problem (maximizing the log likelihood for our mixture model directly) with two easier problems (taking an expectation over our latent variables and maximizing our model parameters based on the complete data, which relies on the current setting of our latent variables). We alternate between the two easier problems, executing each of them until we reach a point of convergence or decide that we've done enough.

We'll walk through the details of each of these steps and then tie them together with the complete algorithm.

\subsection{Expectation Step}
\textbf{TODO: explain what we're doing in the expectation step, what the probability notation is, why it's tractable, why we then refer to the values as 'q' values, the fact that there is a 'q' value for each data point (versus a global parameter)}

The purpose of the expectation step (sometimes just referred to as the 'E'-step) in the EM algorithm is to set the most likely values for our latent variables $z_n$. In an ideal world, we would know what these classes are, and if we did, we could simply optimize our complete-data log likelihood directly as we've seen above. However, since we don't get to observe these values, one way we can get around this is just to compute the expectation of the latent variables. Let's consider what this looks like using a concrete example.

Let's say our data points $\textbf{x}_n$ can come from one of three classes. Then, we can represent the latent variable $\textbf{z}_n$ associated with each data point using a one-hot encoded vector. For example, if $\textbf{z}_n$ came from class $C_1$, we would denote this:
\begin{align*}
    \textbf{z}_n = 
        \begin{bmatrix}
            1 \\
            0 \\
            0 \\
        \end{bmatrix}
\end{align*}
As we've already described, we don't know the value of this latent variable. Instead, we will compute its conditional expectation based on the current setting of our model parameters and our observed data $\textbf{x}_n$. We denote these expected latent variables $\textbf{q}_n$, and we calculate them as follows:
\begin{align*}
    \textbf{q}_n = \mathbb{E}[\textbf{z}_n | \textbf{x}_n] &= \begin{bmatrix}
            p(\textbf{z}_n = C_1 | \textbf{x}_n; \boldsymbol{\theta}, \textbf{w}) \\
            p(\textbf{z}_n = C_2 | \textbf{x}_n; \boldsymbol{\theta}, \textbf{w}) \\
            p(\textbf{z}_n = C_3 | \textbf{x}_n; \boldsymbol{\theta}, \textbf{w}) \\
        \end{bmatrix} \\
        &\propto \begin{bmatrix}
            p(\textbf{x}_n | \textbf{z}_n = C_1; \textbf{w})p(\textbf{z}_n = C_1; \boldsymbol{\theta}) \\
            p(\textbf{x}_n | \textbf{z}_n = C_2; \textbf{w})p(\textbf{z}_n = C_2; \boldsymbol{\theta}) \\
            p(\textbf{x}_n | \textbf{z}_n = C_3; \textbf{w})p(\textbf{z}_n = C_3; \boldsymbol{\theta}) \\
        \end{bmatrix} \\
\end{align*}
Notice that we can switch from proportionality of our $\textbf{q}_n$ values to actually probabilities by simply taking a softmax over the unnormalized values. Then, our $\textbf{q}_n$ values will look something like the following, where a larger number indicates that we have a stronger belief that the data point $\textbf{x}_n$ came from that class:
\begin{align*}
    \textbf{q}_n = \begin{bmatrix}
            0.8 \\
            0.1 \\
            0.1 \\
        \end{bmatrix}
\end{align*}
There are two important things to note about the expectation step. First, notice that the model parameters ($\boldsymbol{\theta}$ and \textbf{w}) are held fixed during this step. We're computing the expectation of our latent variables based on the current setting of those model parameters. Those parameters are randomly initialized if this is our first expectation round.

Second, notice that we have a value of $\textbf{q}_n$ to compute for every data point $\textbf{x}_n$ in our data set during this step. As a result, these are sometimes called \textit{local} variables, since there is one assigned to each data point. This is in contrast to our model parameters, which are considered \textit{global} variables. The size of the global model parameters doesn't fluctuate based on the size of our data set.

After performing the E-step, we now have an expectation for our latent variables, given by $\textbf{q}_n$. In the maximization step, which we descrbie next, we use these $\textbf{q}_n$ values to optimize our model parameters.

\subsection{Maximization Step}
\textbf{TODO: outline the max step, updating our model parameters based on the current q values}
After the expectation step, we have $\textbf{q}_n$ values associated with each data point $\textbf{x}_n$, which describe our belief that each data point came from each class $C_i$. Now that we have these updated `class assignments', it's possible for us to update our model parameters $\boldsymbol{\theta}$ and $\textbf{w}$.

Recall that it's possible for us to optimize these parameters because the complete-data log likelihood is tractable (as we saw above, we no longer need to sum over the possible classes inside of the logarithm). Although we do not have the actual complete-data (we don't actually know the $\textbf{z}_n$ values), we have an estimation of the complete-data through our $\textbf{q}_n$ values! We use these values of $\textbf{q}_n$ to make the optimization of our model parameters tractable.

Notice that our $\textbf{q}_n$ values are `soft' assignments - meaning that unlike the $\textbf{z}_n$ values, which are one-hot encodings of assignments to a class, the $\textbf{q}_n$ values have a probability that each data point $\textbf{x}_n$ came from each class. Fortunately, this does not affect our maximization, which starts with out complete-data log likelihood:
\begin{align*}
    \log p(\textbf{X}, \textbf{Z}) = \sum_{n=1}^{N} \log p(x_n | z_n = c; w) + \log p(z_n = c; \theta) \\
\end{align*}
Applying the expectation with respect to $\textbf{z}_n | \textbf{x}_n$, which will require the $\textbf{q}_n$ computed in the expectation step:
\begin{align*}
    \mathrm{E}_{\textbf{z}_n | \textbf{x}_n} [\log p(\textbf{X}, \textbf{Z})] &= \sum_{c=1}^{C} p(z_{n, c} | \textbf{x}) \sum_{n=1}^{N} \log p(x_n | z_n = c; w) + \log p(z_n = c; \theta) \\
    &= \sum_{c=1}^{C} q_{n, c} \sum_{n=1}^{N} \log p(x_n | z_n = c; w) + \log p(z_n = c; \theta) \\
\end{align*}
Notice the crucial difference between this summation and that of Equation \ref{intractable-log-likelihood}: the summation over the classes is now outside of the logarithm! Recall that using the log-likelihood directly was intractable initially because the summation over the classes was inside of the logarithm. Notice that this maximization became possible by taking the expectation of our latent variables (using the values we computed in the E-step), which pushed the summation over the classes outside of the logarithm.

We can now complete the M-step by performing the maximization of our now tractable equation with respect to our model parameters $\boldsymbol{\theta}$ and $\textbf{w}$. We simply take the derivative with respect to the parameter of interest, set to 0, solve, and update the parameter with the result.

\subsection{Full EM Algorithm}
Now that we have a grasp on the high level goal of the EM algorithm, as well as the expectation and maximization steps individually, we are ready to put everything together to describe the entire EM algorithm.

\begin{itemize}
    \item[1.] Begin by initializing our model parameters $\textbf{w}$ and $\boldsymbol{\theta}$. We can assign these values randomly. Since the EM algorithm is performed over a number of iterative steps, we will denote these initial parameters values $\textbf{w}^{(0)}$ and $\boldsymbol{\theta}^{(0)}$. We will increment those values as the algorithm proceeds.
    \item[2.] E step: compute the values of $\textbf{q}$ based on the current setting of our model parameters.
    \begin{align*}
        \textbf{q}_n = \mathbb{E}[\textbf{z}_n | \textbf{x}_n] = \begin{bmatrix}
                p(\textbf{z}_n = C_1 | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \textbf{w}^{(i)}) \\
                \vdots \\
                p(\textbf{z}_n = C_k | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \textbf{w}^{(i)}) \\
            \end{bmatrix} 
            \propto \begin{bmatrix}
            p(\textbf{x}_n | \textbf{z}_n = C_1; \textbf{w}^{(i)})p(\textbf{z}_n = C_1; \boldsymbol{\theta}^{(i)}) \\
            \vdots \\
            p(\textbf{x}_n | \textbf{z}_n = C_k; \textbf{w}^{(i)})p(\textbf{z}_n = C_k; \boldsymbol{\theta}^{(i)}) \\
        \end{bmatrix} \\
    \end{align*}
    \item[3.] M step: compute the values of $\textbf{w}$ and $\boldsymbol{\theta}$ that maximize our expected complete data log likelihood for the current setting of the values of $\textbf{q}$.
    \begin{align*}
        \textbf{w}^{(i + 1)} = \underset{\textbf{w}}{\arg\max} \; \mathbb{E}_{\textbf{q}}[\log p(\textbf{X}, \textbf{Z})]
    \end{align*}
    \item[4.] Return to step 2, repeating this cycle until our likelihood converges.
\end{itemize}

\subsection{Connection to K-Means Clustering}
At this point, it's worth considering the similarity between the EM algorithm and another coordinate ascent algorithm that we've already considered in the context of clustering: K-Means.

Notice that K-Means proceeds according to a similar iterative algorithm: we first make hard assignments of data points to the existing cluster centers, and then we update the cluster centers based on the most recent assignments.

In fact, the main difference between K-Means clustering and the EM algorithm is that:

\begin{itemize}
    \item[1.] In the EM setting, we make soft cluster assignments through our $\textbf{q}$ values, rather than definitively assigning each data point to only one cluster.
    \item[2.] The EM algorithm is able to take advantage of arbitrary probability distributions to capture the behavior of the data, whereas K-Means clustering relies only on distance measurements to make assignments and update cluster centers.
\end{itemize}

In summary, K-Means is simply a restricted form of the EM algorithm.

\subsection{Example: Dice Rolling Mixture of Multinomials}
\textbf{TODO: run through an example implementation for the coin tossing setup}

Consider the following example scenario: we have two biased dice (with 6 faces) and 1 biased coin (with 2 sides). Data is generated as follows: first, the biased coin is flipped. If it lands heads, Dice 1 is rolled. If it lands heads, Dice 2 is rolled. We record the result of the dice roll, but that is our only observation. For example, our observations may look like:
\begin{align*}
    1, 5, 3, 4, 2, 2, 3, 1, 6
\end{align*}
We're going to try to infer the parameters of each of the dice based on these observation.

Let's consider how this scenario fits into our idea of a mixture model. First, the latent variable $\textbf{z}_n$ has a natural interpretation as being which Dice was rolled for the $n^{th}$ observed data point $\textbf{x}_n$. We can represent $\textbf{z}_n$ using a one-hot vector, so that if the $n^{th}$ data point came from Dice 1, we'd denote that:
\begin{align*}
    \textbf{z}_n =
        \begin{bmatrix}
            1 \\
            0 \\
        \end{bmatrix} \\
\end{align*}
Unsurprisingly, we denote the probability vector associated with the biased coin as $\boldsymbol{\theta} \in \mathbb{R}^{2}$, with $\theta_1$ being the probability of the biased coin landing heads and $\theta_2$ being the probability of the biased coin landing tails.
Furthermore, we need parameters to describe the behavior of our biased dice. We can use $\boldsymbol{\pi_1}, \boldsymbol{\pi}_2 \in \mathbb{R}^{6}$, where each 6-dimensional parameter vector describes the probability that each dice lands on that face. Notice that we now have our model parameters $\textbf{w} = \{\boldsymbol{\theta}, \boldsymbol{\pi}_1, \boldsymbol{\pi}_2 \}$.
Now that we have our model parameters and an understanding of how this problem fits into the mixture model paradigm, we can proceed with using the EM algorithm.
We start by initializing our parameters $\textbf{w}^{(0)}$.
Next, we need to compute our $\textbf{q}_n$ values (the expectation step). Recall that the formula for this is given by:
\begin{align} \label{E-for-multinomial}
    \textbf{q}_n &= \begin{bmatrix}
                p(\textbf{z}_n = C_1 | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \textbf{w}^{(i)}) \\
                p(\textbf{z}_n = C_2 | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \textbf{w}^{(i)}) \\
            \end{bmatrix} \\
            &\propto \begin{bmatrix}
            p(\textbf{x}_n | \textbf{z}_n = C_1; \textbf{w}^{(i)})p(\textbf{z}_n = C_1; \boldsymbol{\theta}^{(i)}) \\
            p(\textbf{x}_n | \textbf{z}_n = C_2; \textbf{w}^{(i)})p(\textbf{z}_n = C_2; \boldsymbol{\theta}^{(i)}) \\
        \end{bmatrix} \\
        &\propto \begin{bmatrix}
            (\pi_{11})^{x_{n,1}}(\pi_{12})^{x_{n,2}}(\pi_{13})^{x_{n,3}}(\pi_{14})^{x_{n,4}}(\pi_{15})^{x_{n,5}}(\pi_{16})^{x_{n,6}}\theta_1 \\
            (\pi_{21})^{x_{n,1}}(\pi_{22})^{x_{n,2}}(\pi_{23})^{x_{n,3}}(\pi_{24})^{x_{n,4}}(\pi_{25})^{x_{n,5}}(\pi_{26})^{x_{n,6}}\theta_2 \\
        \end{bmatrix} \\
\end{align}
where $x_{n,1}, ..., x_{n,6}$ denotes what was rolled for the data point $\textbf{x}_n$.
After computing the values of $\textbf{q}_n$, we are ready to perform the maximization step for our model parameters. Recall that we are maximizing the expected complete data log likelihood, which takes the form:
\begin{align} \label{M-for-multinomial}
    \mathbb{E}_{\textbf{q}}[\log p(\textbf{X}, \textbf{Z})] &= \mathbb{E}_{\textbf{q}} \bigg[\sum_{n=1}^{N} \log p(\textbf{z}_n) + \log p(\textbf{x}_n | \textbf{z}_n)\bigg] \\ 
    &= \sum_{n=1}^{N} \mathbb{E}_{\textbf{q}} \bigg[ \log p(\textbf{z}_n) + \log p(\textbf{x}_n | \textbf{z}_n)\bigg] \\ 
    &= \sum_{n=1}^{N} \sum_{k=1}^{2} q_{n, k} \bigg( \log \theta_k + \sum_{j=1}^{6} x_{n, j} \log \pi_{k, j} \bigg) \\ 
\end{align}
Note that to maximize the expected complete data log likelihood, it's necessary to introduce Lagrange multipliers to enforce the constraints $\sum_{k} \theta_k = 1$ and $\sum_{j} \pi_{k, j} = 1$. After doing this, we recover the following update equations for our model parameters:
\begin{align*}
    \theta_{k}^{(i)} \leftarrow \frac{\sum_{n=1}^{N} q_{n, k}}{N}
\end{align*}
\begin{align*}
    \pi_{k}^{(i)} \leftarrow \frac{\sum_{n=1}^{N} q_{n, k} \textbf{x}_{n}}{\sum_{n=1}^{N} \sum_{j=1}^{6} q_{n, k} x_{n, j}}
\end{align*}

We now have everything we need to perform EM for this setup. After initializing our parameters $\textbf{w}^{(0)}$, we perform the E step by evaluating \ref{E-for-multinomial}. After calculating our values of $\textbf{q}_n$ in the E step, we update our parameters $\textbf{w} = \{\boldsymbol{\theta}, \boldsymbol{\pi}_1, \boldsymbol{\pi}_2 \}$ in the M step by maximizing \ref{M-for-multinomial} with respect to $\boldsymbol{\theta}, \boldsymbol{\pi}_1, \boldsymbol{\pi}_2$. We perform these two steps iteratively, until convergence of our parameters.

\section{Gaussian Mixture Models (GMM)}
\textbf{TODO: explain the gaussian mixture model setup, what an example use case for it is, what the updates look like}

\section{Admixture Models: Latent Dirichlet Allocation (LDA)}
\textbf{TODO: explain what an admixture model is, how it's just an extension of a mixture model. explain the setup for the document example and how we optimize that model.}

\section{Conclusion}
\textbf{TODO: conclude our mixture model chapter}
