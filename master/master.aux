\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{loc}{\contentsline {chapter}{Linear Regression}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Linear Regression}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction and Motivation}{1}}
\newlabel{def:regression}{{2.1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Examples of Regression}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Linear Regression}{1}}
\newlabel{def:linear-regression}{{2.1.2}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Technical}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Merging of Bias}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Data set with clear trend.}}{3}}
\newlabel{fig:simple-lin-reg}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Visualization of Linear Regression}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Data set with clear trend, best fitting line included.}}{4}}
\newlabel{fig:simple-lin-reg-w-line}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Finding the Best Fitting Line: Loss}{4}}
\newlabel{def:loss}{{2.2.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Least Squares Loss}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Quadratic function with clear optimum at $x=2$, where the derivative of the function is 0.}}{5}}
\newlabel{fig:quad-deriv-at-2}{{2.3}{5}}
\newlabel{least-squares-loss-fn}{{2.5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Optimal Weights via Matrix Differentiation}{6}}
\newlabel{der:least-squares-derivation}{{2.2.1}{6}}
\newlabel{least-squares-solving-for-w}{{2.8}{6}}
\newlabel{least-squares-solved-for-w}{{2.9}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Linear Regression as Projection}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Basis Functions}{7}}
\newlabel{def:basis-fn}{{2.2.2}{7}}
\newlabel{least-squares-loss-fn-w-basis}{{2.10}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Data with no basis function applied.}}{8}}
\newlabel{fig:lin-reg-no-basis-fn}{{2.4}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Regularization}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Data with no basis function applied, attempt to fit a line.}}{9}}
\newlabel{fig:lin-reg-no-basis-fn-fitted}{{2.5}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Data with square root basis function applied.}}{9}}
\newlabel{fig:lin-reg-w-basis-fn-fitted}{{2.6}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Data set with a clear trend and Gaussian noise.}}{10}}
\newlabel{fig:data-set-scattered}{{2.7}{10}}
\newlabel{def:regularization}{{2.2.3}{10}}
\newlabel{least-squares-loss-fn-w-regularization}{{2.11}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Natural fit for this data set.}}{11}}
\newlabel{fig:data-set-natural-fit}{{2.8}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Unnatural fit for this data set.}}{11}}
\newlabel{fig:data-set-unnatural-fit}{{2.9}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Effect of different regularization parameter values on final regression solution.}}{12}}
\newlabel{fig:ridge-reg-diff-values}{{2.10}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Bias-Variance Tradeoff and Decomposition}{12}}
\newlabel{def:bias-variance-tradeoff}{{2.2.4}{12}}
\newlabel{der:bias-variance-decomp}{{2.2.2}{13}}
\newlabel{bias-variance-intermediate-1}{{2.12}{13}}
\newlabel{bias-variance-intermediate-2}{{2.13}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Bias and variance both contribute to the overall error of our model.}}{15}}
\newlabel{fig:bias-vs-variance}{{2.11}{15}}
\newlabel{def:overfitting}{{2.2.5}{15}}
\newlabel{def:underfitting}{{2.2.6}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.10}Cross-Validation}{15}}
\newlabel{def:cross-validation}{{2.2.7}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.11}Bayesian Linear Regression}{16}}
\newlabel{normal-over-w}{{2.14}{16}}
\newlabel{der:mle-bayesian}{{2.2.3}{17}}
\newlabel{bayesian-solved-for-w}{{2.15}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.12}Regularization Interpretation}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Ridge Regression}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Lasso Regression}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Elastic Net}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.13}Bayesian Regularization}{18}}
\newlabel{bayesian-regularization-section}{{2.2.13}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Form of the ridge (blue) and lasso (red) regression functions.}}{19}}
\newlabel{fig:ridge-and-lasso-reg-fn-form}{{2.12}{19}}
\newlabel{der:bayesian-regularization-derivation}{{2.2.4}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.14}Predictive Distribution}{20}}
\newlabel{der:posterior-predictive-derivation}{{2.2.5}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Conclusion}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Practice Problems}{21}}
