\chapter{Clustering}
In this chapter, we will explore a technique known as clustering. This represents our first foray into unsupervised machine learning techniques. Unlike the previous four chapters, where we explored techniques that assumed a dataset of inputs and targets, with the goal of eventually making predictions over unseen data, we now don't have an explicit target in mind. These techniques are instead motivated by the goal of uncovering structure in our data. Identifying clusters of similar data points is a useful and ubiquitous unsupervised technique.

\section{Motivation}
The goals of an unsupervised technique like clustering can be very broad. We don't have a specific task in mind; rather, we are trying to uncover more information about a potentially opaque data set. There are a variety of clustering algorithms that we have at our disposal. That being said, one thing that will remain relatively constant between these different methods is that we need to decide on some measure of \textit{distance} between data points.

The concept of distance is simply the idea that two data points should have some measure of how `different' they are from each other. Then, we can use these measurements to determine which data points are relatively the most similar, and thus should be clustered together. One common example of distance that will look familiar is known as L2 distance, which for two data points $\textbf{x}$ and $\textbf{x}'$ is given by:
\begin{equation} \label{l2-distance}
	|| \textbf{x} - \textbf{x'} ||_{L2} = \sqrt{\sum_{d=1}^{D} (\textbf{x}_{d} - \textbf{x'}_{d})^{2}}
\end{equation}
where $D$ is the dimensionality of our data. Notice the similarity of this expression to L2 regularization.

This is just one example of distance for data points that live in a $D$-dimensional Euclidean space. There are other measurements of distance we could have chosen, and for some types of data (such as discrete features), we would need to select a different distance metric. Furthermore, the metrics we choose to use will have an impact on the final results of our clustering.

\begin{mlcube}{Clustering}
Clustering algorithms can operate over inputs in both a continuous and discrete feature space, are fully unsupervised, and for the techniques we will explore in this chapter, are non-probabilistic.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous/Discrete & Unsupervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

\subsection{Applications}
There are many reasons why we might want to be able to separate our data based on similarity. For organizational purposes, it's convenient to have different classes of data. It might make it easier for a human being to sift through a large trove of data if it's first separated based on similarity. It might also be an important preprocessing step for other data science techniques; for example, we may be able to create additional features for a supervised learning techniques by clustering our input data. It might be useful to separate our data so that we can get some idea of what features make the different data points distinct from one another. It might even give us some idea of how many distinct types of data we have in our data set. Here are a few specific examples of use cases for clustering:

\begin{enumerate}
    \item Determining the number of distinct phenotypes in a population.
    \item Organizing images into buckets according to the types of scenes present.
    \item Grouping financial data to be used as a feature for anticipating extreme market events.
    \item Identifying the attributes that most heavily distinguish different DNA sequences.
\end{enumerate}

As we mentioned above, there are different methods available for performing clustering. In this chapter, will explore two of the most common techniques: K-Means Clustering and Hierarchical Agglomerative Clustering. Additionally, we will touch on the different flavors available within each of these larger techniques.

\section{K-Means Clustering}
The high level procedure behind K-Means Clustering (known informally as k-means) is as follows:

\begin{enumerate}
    \item Randomly initialize cluster centers in our feature space.
    \item Using a distance metric of your choosing, assign each data point to the closest cluster.
    \item Update the cluster centers by averaging the data points assigned to each cluster.
    \item Repeat steps 1. and 2. until convergence.
\end{enumerate}

In the case where we are using the L2 distance metric, this is known as \textit{Lloyd's algorithm}, which we derive in the next section.

\subsection{Lloyd's Algorithm Derivation}
derive Lloyd's algorithm here, discuss local optimum idea (it will converge), that it's NP hard to actually find global optimmum, complexity of Lloyd's, mention that it's a good idea to be standardizing our data

Lloyd's algorithm, named after Stuart P. Lloyd who first suggested the algorithm in 1957, optimizes our cluster assignments via a technique known as coordinate descent, which we will learn more about in later chapters. It begins by defining a loss function for our current assignment of data points to clusters:
\begin{equation} \label{clustering-loss-fn}
	\mathcal{L}(\boldsymbol{\mu}, \textbf{X}, \textbf{R}) = \sum_{n=1}^{N} \sum_{c=1}^{C} \textbf{R}_{nc} \sqrt{\sum_{d=1}^{D} (\textbf{X}_{nd} - \boldsymbol{\mu}_{cd})^{2}}
\end{equation}
where $\textbf{X}$ is our $N$x$D$ data set ($N$ is the number of data points and $D$ is the dimensionality of our data), $\boldsymbol{\mu}$ is the $C$x$D$ matrix of cluster centers ($C$ is the number of clusters we chose), and $\textbf{R}$ is our $N$x$C$ matrix of \textit{responsibility vectors}. These are one-hot encoded vectors (one per data point), where the 1 is in the position of the cluster to which we assigned the $n$th data point. TODO: definition for responsibility vectors? TODO: change the notation for this loss function?

\subsection{Example of Lloyd's}
Do a random initialization and walk through an example of how it works
\subsection{Number of Clusters}
How do we actually choose the number of clusters, using the knee approach vs the loss
\subsection{Initialization and K-Means++}
good ways to initialize our data points to uncover a good result, performing K-means multiple times to settle on a final result, using K-Means++ initialization tactic
\subsection{K-Medoids Alternative}
explain how sometimes our center points can't actually be averaged, in which case we need to use something like k-medoids

\section{Hierarchical Agglomerative Clustering}