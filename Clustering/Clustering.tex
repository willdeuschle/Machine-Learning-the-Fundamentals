\chapter{Clustering}
In this chapter, we will explore a technique known as clustering. This represents our first foray into unsupervised machine learning techniques. Unlike the previous four chapters, where we explored techniques that assumed a dataset of inputs and targets, with the goal of eventually making predictions over unseen data, we no longer have an explicit target in mind. These techniques are instead motivated by the goal of uncovering structure in our data. Identifying clusters of similar data points is a useful and ubiquitous unsupervised technique.

\section{Motivation}
The reasons for using an unsupervised technique like clustering are broad. We often don't have a specific task in mind; rather, we are trying to uncover more information about a potentially opaque data set. This information typically indicates some measure of \textit{distance} between our data points. While there are a variety of clustering algorithms available, the importance of this distance measurement is consistent between them.

Distance is meant to capture how `different' two data points are from each other. Then, we can use these distance measurements to determine which data points are similar, and thus should be clustered together. A common distance measurement for two data points $\textbf{x}$ and $\textbf{x}'$ is given by:
\begin{equation} \label{l2-distance}
	|| \textbf{x} - \textbf{x'} ||_{L2} = \sqrt{\sum_{d=1}^{D} (\textbf{x}_{d} - \textbf{x'}_{d})^{2}}
\end{equation}
where $D$ is the dimensionality of our data. This is known as \textbf{L2 distance}, and you can likely see the similarity to L2 regularization.

There are a variety of distance measurements available for data points living in a $D$-dimensional Euclidean space, but for other types of data (such as data with discrete features), we would need to select a different distance metric. Furthermore, the metrics we choose to use will have an impact on the final results of our clustering.

\begin{mlcube}{Clustering}
Clustering algorithms can operate on both continuous and discrete feature spaces, are fully unsupervised, and are non-probabilistic for the techniques we explore in this chapter.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous/Discrete & Unsupervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

\subsection{Applications}
There are many reasons why we might separate our data by similarity. For organizational purposes, it's convenient to have different classes of data. It can be easier for a human to sift through data if it's loosely categorized beforehand. It may be a preprocessing step for an inference method; for example, by creating additional features for a supervised technique. It can help identify which features make our data points most distinct from one another. It might even provide some idea of how many distinct data types we have in our set. Here are a few specific examples of use cases for clustering:

\begin{enumerate}
    \item Determining the number of phenotypes in a population.
    \item Organizing images into folders according to scene similarity.
    \item Grouping financial data as a feature for anticipating extreme market events.
    \item Identifying similar individuals based on DNA sequences.
\end{enumerate}

As we mentioned above, there are different methods available for clustering. In this chapter, we will explore two of the most common techniques: K-Means Clustering and Hierarchical Agglomerative Clustering. We also touch on the flavors available within each of these larger techniques.

\section{K-Means Clustering}
The high level procedure behind K-Means Clustering (known informally as k-means) is as follows:

\begin{enumerate}
    \item Initialize cluster centers by randomly selecting points in our data set.
    \item Using a distance metric of your choosing, assign each data point to the closest cluster.
    \item Update the cluster centers by averaging the data points assigned to each cluster.
    \item Repeat steps 1 and 2 until convergence.
\end{enumerate}

In the case where we are using the L2 distance metric, this is known as \textit{Lloyd's algorithm}, which we derive in the next section.

\subsection{Lloyd's Algorithm}
Lloyd's algorithm, named after Stuart P. Lloyd who first suggested the algorithm in 1957, optimizes our cluster assignments via a technique known as coordinate descent, which we will learn more about in later chapters.

\begin{derivation}{Lloyd's Algorithm Derivation}{lloyds-algorithm-derivation}
	We begin by defining a loss function for our current assignment of data points to clusters:
	\begin{equation} \label{clustering-loss-fn}
		\mathcal{L}(\textbf{X}, \big\{\boldsymbol{\mu}\big\}_{c=1}^{C}, \big\{\textbf{r}\big\}_{n=1}^{N}) = \sum_{n=1}^{N} \sum_{c=1}^{C} r_{nc} (\textbf{x}_{n} - \boldsymbol{\mu}_{c})^{2}
	\end{equation}
	where $\textbf{X}$ is our $N$x$D$ data set ($N$ is the number of data points and $D$ is the dimensionality of our data), $\big\{\boldsymbol{\mu}\big\}_{c=1}^{C}$ is the $C$x$D$ matrix of cluster centers ($C$ is the number of clusters we chose), and $\big\{\textbf{r}\big\}_{n=1}^{N}$ is our $N$x$C$ matrix of \textit{responsibility vectors}. These are one-hot encoded vectors (one per data point), where the 1 is in the position of the cluster to which we assigned the $n$th data point. \newline

	Next, we adjust our responsibility vectors to minimize each data point's distance from its cluster center. Formally:
	\begin{equation} \label{responsibility-vector-update}
		r_{nc} = \begin{cases}
		 	= 1 & \text{if $c = \underset{c'}{\arg\min} ||\textbf{x}_{n} - \boldsymbol{\mu}_{c'}||$} \\
			= 0 & \text{otherwise} \\
		\end{cases}
	\end{equation}
	After updating our responsibility vectors, we now wish to minimize our loss by updating our cluster centers $\boldsymbol{\mu}_{c}$. The cluster centers which minimize our loss can be computed by taking the derivative of our loss with respect to $\boldsymbol{\mu}_{c}$, setting equal to 0, and solving for our new cluster centers $\boldsymbol{\mu}_{c}$:
	\begin{equation} \label{update-cluster-centers}
		\begin{aligned}
			\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_{c}} = -2 \sum_{n=1}^{N} r_{nc} (\textbf{x}_{n} - \boldsymbol{\mu}_{c}) \\
			\boldsymbol{\mu}_{c} = \frac{\sum_{n=1}^{N} r_{nc} \textbf{x}_{n}}{\sum_{n=1}^{N} r_{nc}}
		\end{aligned}
	\end{equation}
	Intuitively, this is the average of all the data points $\textbf{x}_{n}$ assigned to the cluster center $\boldsymbol{\mu}_{c}$. \newline

	We then update our responsibility vectors based on the new cluster centers, update the cluster centers again, and continue this cycle until we have converged on a stable set of cluster centers and responsibility vectors.
\end{derivation}

TODO: pickup proofreading here \newline
Note that while Lloyd's algorithm is guaranteed to converge, it is only guaranteed to converge to a locally optimum solution. Finding the globally optimum set of assignments and cluster centers is an NP-hard problem. Thus, one common strategy is to execute Lloyd's algorithm several times with different random initializations of our cluster centers, selecting the final assignment that minimizes loss. Furthermore, to avoid nonsensical solutions due to scale mismatch between features, it makes sense to standardize our data in a preprocessing step (subtracting the mean and dividing by the standard deviation across each feature).

TODO: mention time complexity of Lloyd's?

\subsection{Example of Lloyd's}
TODO: Do a random initialization and walk through an example of how it works

\begin{example}{Lloyd's Algorithm Example}{lloyds-algorithm-example}
	Let's say we start with a data set of size $N=6$. Each data point is two-dimensional, with each feature taking on a value between -3 and 3. Here is a table and graph of our data points, labelled A through F:

	TODO: table graph here.

	Let's say we wish to have 2 cluster centers. We then randomly initialize those cluster centers by selecting two data points. Let's say we select B and F. We identify our cluster centers with a red and green `X':

	TODO: graph here with the cluster centers now in it

	We now begin Lloyd's algorithm by assigning each data point to its closest cluster center:

	TODO: graph and table

	We then update our cluster centers by averaging the data point assigned to each:

	TODO: graph and table.

	We proceed like this, updating our cluster centers and assignments, until convergence. At convergence, we've achieved these cluster centers and assignments:

	TODO: final graph and table of assignments.

	Note that for this random initialization of cluster centers, we deterministically identified the locally optimal set of assignments and cluster centers. For a different initialization, we may have finished with a different result. However, for a given specific initialization, running Lloyd's algorithm will always identify the same set of assignments and cluster centers.
\end{example}

\subsection{Number of Clusters}
You may have wondered about a crucial detail we've left out until now: how do we choose the proper number of clusters for our data set? There doesn't actually exist a `correct' number of clusters. The fewer clusters we have, the larger our loss will be, and as we add more clusters, our loss will get strictly smaller. That being said, there is certainly a tradeoff to be made here.

Having a single cluster is obviously useless - we will group our entire data set into the same cluster. Having $N$ clusters is equally useless - each data point gets its own cluster.

One popular approach to identifying a good number of clusters is to perform K-Means with a varying number of clusters, and then to plot the number of clusters against the loss. Typically, that graph will look like Figure TODO: ref figure here.

TODO: put graph here.

Notice that at $x=...$ clusters, there appears to be a slight bend in the decrease of our loss. This is sometimes known as the \textbf{knee}, and it is common to choose the number of clusters to be where the knee occurs.

Intuitively, the idea here is that up to a certain point, adding another cluster significantly decreases the loss by more properly grouping our data points. However, eventually the benefit of adding another cluster stops being quite so significant. At this point, we have identified a natural number of groups for our data set.

\subsection{Initialization and K-Means++}
good ways to initialize our data points to uncover a good result, performing K-means multiple times to settle on a final result, using K-Means++ initialization tactic

Up until now, we have assumed that we should just randomly initialize our cluster centers and then execute Lloyd's algorithm until we converge. We also suggested that since Lloyd's algorithm only produces a local minimum, it makes sense to perform several random initializations before settling on the most optimal assignment we've identified.

While this is a viable way to perform K-Means, there are other ways of initializing our original cluster centers that can help us find more optimal results without needing so many random re-initializations. One of those techniques is known as \textbf{K-Means++}.

The idea behind K-Means++ is that our cluster centers will typically be spread out when we've converged. As a result, it might not make sense to entirely randomly initialize those cluster centers. For example, Figure TODO: ref figure here would be a poor initialization.

TODO put figure of bad initialization here.

But it would be just as likely to start with that random initialization as to start with the initialization in Figure TODO: ref figure here.

TODO put figure of good initialization here.

We can use the hint that we want our centers somewhat spread out to find a better random initialization. This is where the initialization algorithm presented by K-Means++ comes in.

For K-Means++, we choose the first cluster center by randomly selecting a point in our data set, same as before. However, for all subsequent cluster center initializations, we select points in our data set with probability proportional to the squared distance from their nearest cluster center. The effect of this is that we end up with a set of initializations that are relatively far from one another, as in Figure TODO: ref figure here.

\subsection{K-Medoids Alternative}
Going back to the cluster center update step presented in the Lloyd's algorithm derivation, we see that we are averaging the data points assigned to each cluster to compute the new cluster centers. Note that in some cases, this averaging step doesn't actually make sense (for example, if we have categorical variables as part of our feature set). In these cases, we can use an alternative algorithm known as \textbf{K-Medoids}. The idea behind K-Medoids is simple: instead of averaging the data points assigned to that cluster, update the new cluster center to be a data point assigned to that cluster (ideally the one that is most like all the others).

\section{Hierarchical Agglomerative Clustering}
The motivating idea behind K-Means was that we could use a distance measurement to assign data points to a fixed number of clusters, and from there we just needed a methodology to iteratively improve our assignments and cluster locations. Moving on to Hierarchical Agglomerative Clustering, commonly referred to as HAC (pronounced "hack"), the big idea is now that we can construct a tree over our data set that describes the relationship between our data points. These trees are known as \textit{dendrograms}, and look like what's found in Figure TODO ref figure here

TODO put dendrogram figure here.

Notice that the individual data points are the leaves of our tree, and the trunk is the cluster of data that contains the entirety of our data set. The high level idea behind how we construct this tree is as follows:

\begin{enumerate}
    \item We start with $N$ clusters, one for each data point.
    \item Merge the two `closest' clusters together to reduce the number of clusters by 1. Keep track of the distance between those two clusters.
    \item Repeat step 2 until we're left with only a single cluster.
\end{enumerate}

In the following sections we'll go into greater detail on how to construct the tree (including what is meant by `closest' clusters), establish what the tree gives us in terms of clustering information, and discuss how HAC differs from K-Means.

\subsection{HAC Algorithm}
As described above, HAC proceeds by iteratively merging clusters until we're only left with a single cluster. At the start, each data point is in it's own cluster. We keep track of the distance between the two clusters that we merge at each step, which is what allows us to construct the dendrogram in Figure TODO: ref figure here. To make this process a little more clear, let's perform HAC one step at a time, constructing the dendrogram as we go.

TODO: HAC example here with four data points, probably just use the min linkage criterion.

Notice how the distance between two merged clusters manifests itself as the height of the dendrogram when they were merged (which is why we tracked those distances as we constructed the tree). Notice also that we now have many layers of clustering: if we're only only interested in clusters whose elements are at least $k$ units away from each other, we can `cut' the dendrogram at that height and examine all the clusters that exist below there.

Finally, there was an important piece of this example, that we chose to gloss over, but that is very important to the different clusterings we might find using HAC. For this example, we designated the distance between two clusters to be the minimum distance between any two data points across the clusters. This is what is known as the \textbf{Min-Linkage Criterion}. However, there were certainly other ways we could have computed the distance between clusters. We now turn to these different methods and the properties of clusters they produce next.

\subsection{Linkage Criterion}
Here are a few of the most common linkage criteria. TODO: describe the different types of clusterings that each of these choices tends to produce

\subsubsection{Min-Linkage Criteria}
We've already seen the Min-Linkage Criterion in action from the previous example. Formally, the criterion says that the distance $d_{C, C'}$ between each cluster pair $C$ and $C'$ is given by
\begin{equation} \label{min-linkage-crit}
	d_{C, C'} = \underset{k, k'}{\min} || \textbf{x}_{k} - \textbf{x}_{k'} ||
\end{equation}
where $\textbf{x}_{k}$ are data points in cluster $C$ and $\textbf{x}_{k'}$ are data points in cluster $C'$. After computing these pairwise distances, we choose to merge the two clusters that are closest together.

\subsubsection{Max-Linkage Criterion}
We could also imagine defining the distance $d_{C, C'}$ between two clusters as being the distance between the two points that are farthest apart in each of them. This is known as the Max-Linkage Criterion. The distance between two clusters is then given by:
\begin{equation} \label{max-linkage-crit}
	d_{C, C'} = \underset{k, k'}{\max} || \textbf{x}_{k} - \textbf{x}_{k'} ||
\end{equation}
Like with the Min-Linkage Criterion, after computing these pairwise distances, we choose to merge the two clusters that are closest together.

\readernote{Be careful not to confuse the linkage criterion with which clusters we choose to merge. We \textbf{always} merge the clusters that have the smallest distance between them. What differs is how we compute that distance, which is given by the linkage criterion.}

\subsubsection{Average-Linkage Criterion}
The Average-Linkage Criterion averages the pairwise distance between each point in each cluster. Formally, this is given by:
\begin{equation} \label{avg-linkage-crit}
	d_{C, C'} = \frac{1}{K K'} \sum_{k=1}^{K} \sum_{k'=1}^{K'} || \textbf{x}_{k} - \textbf{x}_{k'} ||
\end{equation}

\subsubsection{Centroid-Linkage Criterion}
The Centroid-Linkage Criterion uses the distance between the centroid of each cluster (which is the average of the data points in each cluster). Formally, this is given by:
\begin{equation} \label{cent-linkage-crit}
	d_{C, C'} = || \frac{1}{K} \sum_{k=1}^{K} \textbf{x}_{k} - \frac{1}{K'} \sum_{k'=1}^{K'} \textbf{x}_{k'} ||
\end{equation}

\subsubsection{Different Linkage Criteria Produce Different Clusterings}
It's important to note that the linkage criterion you choose to use will influence your final clustering results. For example, the min-linkage criteria tends to produce `stringy' clusters, while the max-linkage criteria tends to produce more compact clusters. You can see the difference between the results of these two linkage criteria in Figure (todo figure here).

TODO: do the two different looking clusters for the different linkage criteria here

It's useful to be aware of these different types of results when selecting your linkage criterion for a given problem.

\readernote{You can convince yourself of the different flavors of results for the different linkage criteria. For example, when using the min-linkage criteria, we get these `stringy' results because we're most inclined to extend existing clusters by grabbing whichever data points are closest.}

\subsection{How HAC Differs from K-Means}
Now that we have two different methods for clustering, each with their own respective knobs for modifying the clustering properties, we consider the differences between the two methods.

First of all, as we mentioned above, there is a fundamental difference in determinism between HAC and K-Means. In general, K-Means is probabilistic and needs to be run multiple times to ensure you recover a good result. On the other hand, once you've selected a linkage criterion for HAC, the clusters you uncover are deterministic. You only need to run HAC a single time.

Another difference between HAC and K-Means lives in the assumptions we need to make to run each of them. For K-Means, we need to specify the number of clusters up front before running our algorithm, potentially using a method like the knee-method to decide on the number of clusters. On the other hand, you don't need to know this up front to run HAC. This makes HAC quite simple up front. However, the downside for HAC is that when you wish to present your final clustering results, you need to decide on the max distance between elements in each cluster (so that you can cut the dendrogram).

That final point about HAC, that you need to make a decision about where to cut the dendrogram, also means that after running HAC you have several different clustering options. Furthermore, the dendrogram in and of itself can be a useful tool for visualizing your data. We don't get the same interactivity from K-Means clustering.

\readernote{We often use dendrograms to visualize evolutionary lineage.}