\chapter{Neural Networks}
In this chapter, we explore the oft spoken of neural networks. As we will come to see, neural networks are an extraordinarily flexible model option for solving a variety of different problem types. In fact, this flexibility is both what makes them so widely applicable and so difficult to use properly. We will explore the applications, underlying theory, and training schemes behind neural networks.

\section{Motivation}
As seemingly popular as neural networks have become for recent problem solving, they aren't actually new technology. The first neural networks were described in the early 60s (???), and the only reason they weren't put into practice shortly thereafter was the fact that we didn't yet have access to the large amounts of efficient memory and storage that complex neural network require. Over the last 50 (???) years, and particularly over the last decade with the advent of cloud computing, we now have more and more access to the cheap processing power and storage required to make neural networks a viable option for model building.

\subsection{Applications}
In the previous two chapters, we explored two broad problem types: classification and regression. It's natural to wonder where neural networks fit into these two broad problem types, and the answer is that they are applicable to both. Neural networks are some of the most flexible models we will explore, and that flexibility extends to the types of problems they can be made to handle. Thus, the tasks that we've explored over the last two chapters, such as predicting heights in the regression case or object category in the classification case, can be done by neural networks.

\subsection{Comparison to Other Methods}
As discussed in the previous section, neural networks are flexible enough to be used as models for either regression or classification tasks. This means that every time you're faced with a problem that falls into one of these categories, you have a choice to make between the methods we've already covered or using a neural network. Before we've explored the specifics of neural networks, how can we know at a high level when they will be a good choice for a specific problem?

One simple way to think about this is that if we never needed to use neural networks, we probably wouldn't. In other words, if a problem can be solved effectivly by one of the techniques we already described for regression or classification (such as linear regression, discriminant functions, etc.), we would prefer to use those. The reason is that neural networks are often more memory and processor intensive than these other techniques, and they are much more complex to train and debug.

The flip side of this is that hard problems are often too complex, too ill-specified, or too poorly understood to use a simple regression or classification technique. Indeed, even if you eventually think you will need to use a neural network to solve a given problem, it makes sense to try a simple technique first both to get a baseline of performance and because it may just happen to be good enough.

What is so special about neural networks that they can solve problems that the other techniques we've explored may not be able to? And why are they so expensive? Those are the questions we turn to next.

\subsection{Strengths and Weaknesses}
For problems that fall into the category of regression or classification, we've already discussed the utility of basis functions. Often, a problem that is intractable with our input data as-is will be readily solvable with basis-transformed data. We often select these basis changes using expert knowledge. For example, if we were working with a data set that related to chemical information, and there were certain equations that a chemist told us to be important for the particular problem we were trying to solve, we might include a variety of the transformations that are present in those equations.

However, imagine now that we have a data set with no accompanying expert information. More often than not, complex problem domains don't come with a useful set of suggested transformations. How do we find useful basis functions in these situations? This is exactly the strength of neural networks - they solve for the best basis for a data set!

Neural networks can be used to simultaneously solve for our model parameters as well as the best basis transformations. As we stated above, this makes them exceedingly flexible.

Unfortunately, this flexibility is also the weakness of neural nets. While this flexibility enables us to solve difficult problems, it also opens us up to a host of other problems. Chief among these is the fact that neural networks take a lot of computation to train. This is simply a result of the possible model space being so large - to effectively explore it all takes time and resources. Furthermore, this flexibilty can cause rather severe overfitting if we are not careful.

In summary, the strengths and weaknesses of neural networks stem from the same root cause: model flexibility. It will be our goal then to appropriately harness this property to create useful models.

\subsection{Universal Function Approximation}
The flexibility of neural networks is a well-established phenomenon. In fact, neural networks are what are known as \textit{universal function approximators}. This means that with a large enough network, it is possible to approximate any function. The proof of this is beyond the scope of this textbook, but it provides some context for why flexibility is one of the key attributes of neural networks.

\begin{mlcube}{Neural Networks}
As universal function approximators, neural networks can operate over discrete or continuous inputs. That being said, it's far more efficient and common for them to accept \textbf{continuous} inputs. We primarily use neural networks to solve regression or classification problems, which involve training on input data sets to produce predictions, making them a \textbf{supervised} technique. Finally, while there exist probabilistic extensions for neural networks, they primarily operate in the \textbf{non-probabilistic} setting.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous & Supervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

\section{Feed Forward Networks}
The feed forward neural network is the most fundamental setup for a neural network. Most of the logic behind neural networks can be explained using a feed forward network, with additional features features typically added to form more complex networks. We will explore this basic neural network setup first.

\subsection{Adaptive Basis Functions}
As we mentioned in the introduction, the strength of neural networks is that we can learn an effective basis for our problem domain at the same time as we train the parameters of our model. In fact - learning this basis becomes just another part of our parameter training. Let's make this notion of learning a basis more concrete.

Thinking back to our chapter on linear regression, we were training a model that made predictions using a functional form that looked like this:

\begin{align*}
	y(\textbf{x}, \textbf{w}) = \textbf{w} \boldsymbol{\phi}^{T} = \sum_{d=1}^{D} w_{d} \boldsymbol{\phi}_{d}
\end{align*}

where $\boldsymbol{\phi} = \phi(\textbf{x})$, $\phi$ is the basis transformation function, and $D$ is the dimensionality of our data point.

Typically with this setup, we are training our model to optimize the parameters $\textbf{w}$. This doesn't change with neural networks - we still train to learn those parameters. However, the difference in the neural network setting is that the basis transformation function $\phi$ is not longer fixed. Instead, we parameterize this function and learn its parameters at the same time as we learn the model parameters $\textbf{w}$. This leads to the following functional form for neural networks. We first perform $M$ linear combinations of an input data point $\textbf{x}$:
\begin{equation} \label{basic-nn-form}
	a_{j} = \sum_{d=1}^{D} w_{jd}^{(1)} x_{i} + w_{j0}^{(1)}
\end{equation}
where $j=1..M$ and the notation $(1)$ means that these weights are part of the first `layer' in our network. Notice also that we haven't applied the `bias trick' of appending a $1$ to our data point $\textbf{x}$. We will still apply this trick in general, but we've left it out here to illustrate that the terms $w_{jd}^{(1)}$ are typically referred to as \textit{weights} while the term $w_{j0}^{(1)}$ is known as the \textit{bias}.

The $M$ different values $a_{j}$ that we compute using this equation are what is known as \textit{activations}. We then transform these activations with a non-linear activation function $h(\cdot)$ to give:
\begin{equation} \label{basic-nn-z-outputs}
	z_{j} = h(a_{j})
\end{equation}
These values $z_{j}$ are what is known as \textit{hidden units}. The activation function is often the logistic signmoid function discussed in the previous chapter, but may also be something like a tanh function or rectified linear unit (ReLU). These output units $z_{j}$ become the inputs to the next transformation:
\begin{equation} \label{basic-nn-form-next-layer}
	a_{j+1} = \sum_{d=1}^{D} w_{(j+1)d}^{(2)} x_{i} + w_{(j+1)0}^{(2)}
\end{equation}
Note we can connect many rounds of basis transformation and linear combination, and the number of rounds we choose to include in our networks will be referred to as the number of \textit{layers}. Eventually, we will reach the nodes in the final layer of our network, which after being transformed by their activation function, are typically known as \textit{output activations} denoted $y_{k}$. This final activation function may be the logistic sigmoid function in the binary case or the softmax function in the multiclass scenario.

It is often easier to understand the workings of a feed forward neural network by examining a diagram. The procedure we described in the proceeding paragraphs can be visualized through this network (note that we've chosen to display a two-layer network):

** two layer labeled NN here **

\readernote{Different resources choose to count the number of layers in a neural net in different manners. We've elected to count each layer of connecting weights, thus the two-layer network in Figure (ref figure here). However, some resources will choose to count each layer of nodes (which in this case is three) and still others count only the number of hidden layers (making this a one hidden layer network).}

Combining Figure (ref figure above) and our preceeding functional description, we can describe the operation performed by a three-layer neural network using a single functional transformation:
\begin{equation} \label{full-nn-equation}
	y_{k}(\textbf{x}, \textbf{w}) = \sigma\bigg(\sum_{m=1}^{M}w_{kj}^{(2)} h\bigg(\sum_{d=1}^{D}w_{jd}^{(1)}x_{d} + w_{j0}^{(1)}\bigg) + w_{k0}^{(2)}\bigg)
\end{equation}
where we've elected to make the final activation function the sigmoidal function $\sigma(\cdot)$ as we have a binary output activation layer.

Notice that when written like this, a neural network is simply a non-linear function that transforms and input $\textbf{x}$ into an output $\textbf{y}$ that is controlled by our set of parameters $\textbf{w}$.

Furthermore, it may be clear by now why this simple type of neural networks is referred to as a \textit{feed forward neural network}. If you examine Figure (ref figure here) or Equation \ref{full-nn-equation}, you'll notice that we're simply pushing our input \textbf{x} forward through the network from the first layer to the last layer, hence the name. Assuming we have a fully trained network, we can make predictions on new input data points by propagating our point of interest through the network to generate an output prediction.

Note that we can also simplify this equation by utilizing the bias trick and introducing adding an $x_{0}$ value to each of our data points such that:
\begin{equation*}
	y_{k}(\textbf{x}, \textbf{w}) = \sigma\bigg(\sum_{m=1}^{M}w_{kj}^{(2)} h\bigg(\sum_{d=1}^{D}w_{jd}^{(1)}x_{d}\bigg)\bigg)
\end{equation*}

Finally, it's worth considering that while a neural network is a series of linear combinations, it is special because of the differentiable non-linearities applied at each of the hidden layers. Without these non-linearities, the successive application of different network weights would be equivalent to a single large linear combination.

Now that we understand the basic structure of a neural network, the important question of how we actually train our networks still remains. This is the topic we turn to next.

Discuss the similarities to the perceptron or the fact that its like a multilayer perceptron???

Discuss weight space symmetries here with respect to the structure of a network???

\section{Network Training}
Now that we understand the setup of a basic feed forward network and how it can be used to make predictions, we turn our attention to the training process.

\subsection{Objective Function}
To train our network, it's first necessary to establish a logical error function for our parameters \textbf{w} that we will seek to minimize. Remember that neural networks can be used to solve both regression and classification problems, which means that our choice of error function will be very much dependent on the type of problem we are working on as well as the properties we desire from our error function.

As we've already discussed, for the case of linear regression, a common error function (but certainly not the only possible one) is the least squares loss function:
\begin{equation} \label{least-squares-loss-function}
	E(\textbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \bigg(y(\textbf{x}_{n}, \textbf{w}) - \textbf{t}_{n}\bigg)^{2}
\end{equation}
For a binary classification problem, which we can model using a sigmoidal nonlinearity in our output activation unit, we'll often use the cross-entropy error function given by:
\begin{equation} \label{cross-entropy-loss-function}
	E(\textbf{w}) = - \sum_{n=1}^{N} \bigg(y_{n}\ln{\hat{y}_{n}} + (1 - y_{n})(\ln{(1 - \hat{y}_{n})}\bigg)
\end{equation}
And finally, in the multiclass classification setting produced by a softmax function in our output activation layer, we use the following generalizaton to the cross entropy error function:
\begin{equation} \label{multiclass-cross-entropy-loss-function}
	E(\textbf{w}) = - \sum_{n=1}^{N} \sum_{k=1}^{K} y_{kn} \ln{\bigg(\frac{\text{exp}(a_{k}(\textbf{x}, \textbf{w}))}{\sum_{j=1}^{K}\text{exp}(a_{j}(\textbf{x}, \textbf{w}))}\bigg)}
\end{equation}

\subsection{Optimizing Parameters}
Ultimately, as we've seen in the preceding two chapters, it is our goal to select a value for our model's weight parameters \textbf{w} that minimizes our error function. Remember that we did this previously by taking the derivative of our error function with respect to our weight paramters, setting that expression equal to 0, and solving for \textbf{w}. We were previously able to do that procedure with confidence since we had a convex weight space, which meant the point of minimization for the error function would occur where $\nabla E(\textbf{w}) = 0$.

Unfortunately, for neural networks, this procedure will no longer work quite as well. The error function, which is parameterized by the weight space, will now have a nonlinear dependence on the weight parameters as a result of the nonlinearities applied to the data points as they are pushed forward through our network. The result of this is that there will be many points in the parameter space at which the gradient disappears, corresponding to local minima, maxima, or saddle points. This means that we no longer have an analytical solution, and will instead need to resort to a numerical procedure.

\readernote{The terms \textit{analytical} and \textit{numerical} procedures come up very frequently in machine learning literature. An analytical solution typically utilizes a closed form equation that accepts your model and input data to produces a solution. On the other hand, numerical solutions are those that require some sort of iteration to move toward an ever better solution, eventually stopping once the solution is deemed `good enough'. Analytical solutions are typically more desirable than numerical solutions due to computational efficiency and performance guarantees.}

Most numerical techniques utilize some form of gradient based update as follows:
\begin{equation} \label{generic-numerical-update}
	\textbf{w}^{(t+1)} = \textbf{w}^{(t)} + \nabla \textbf{w}^{(t)}
\end{equation}
where $\textbf{w}^{(t)}$ corresponds to the state of the parameters $\textbf{w}$ at time $t$. The parameter values at time $t=0$ given by $\textbf{w}^{(0)}$ are often initialized randomly, and the updates to the parameters $\nabla\textbf{w}^{(t)}$ are computed differently for different optimization algorithms. \newline
?Discuss the fact that there will be many identical minima given weight space symmetries? \newline
?Offer a visualization of the error function with respect to the parameter space here?

\subsection{Gradient Descent}
We've identified the need to use a numerical optimization technique to compute our weight parameters $\textbf{w}$, and we've specified a generic form of iterative updates to our weight parameters in Equation \ref{generic-numerical-update}. We will now turn to one of the most common procedures for generating the update $\nabla\textbf{w}^{(t)}$, which is known as gradient descent.

The high level idea behind gradient descent is as follows: at each update step, we take a small step in the opposite direction of the gradient of our error function with respect to the weight parameters $\textbf{w}^{(t)}$. Notationally, this looks like the following:
\begin{equation} \label{gradient-descent-equation}
	\textbf{w}^{(t+1)} = \textbf{w}^{(t)} - \eta \nabla E(\textbf{w}^{(t)})
\end{equation}
where $\eta > 0$ is known as the \textit{learning rate}.

\readernote{In general, we want a learning rate that is large enough so that we make progress toward reaching a better solution, but not so large that we take a step that puts us in a worse place in the parameter space than we were at the previous step.}

Why do we take a step in the opposite direction of the gradient of the error function? You can think of the error function as a hill, and the current state of our parameters $\textbf{w}^{(t)}$ is our position on that hill. The gradient tells us the steepest direction of increase in the error function (i.e. it specifies the direction that will make our model worse). Since we want to minimize the error function, we choose to move away from the direction of the gradient, moving our model down the hill towards an area of lower error.

With stochastic gradient descent, we typically cease optimization when our updates become sufficiently small, indicating that we've reached a local minimum. Note that it's likely necessary to run the process of stochastic gradient descent multiple times to settle on a final value for $\textbf{w}$, ideally initializing $\textbf{w}^{(0)}$ to a different starting value each time, because we are optimizing a function with multiple local minima.

Finally, it's important to understand the different means by which we can compute the gradient of our error function at each step. The first way, often called \textit{batch gradient descent}, computes the gradient for our error function at each step using the entire data set. In contrast, the technique known as \textit{stochastic gradient descent} (also known as SGD) utilzies a subset of the data points at each step to compute the gradient, sometimes just one. Stochastic gradient descent is typically a more popular technique for several reasons. First, the computation time is often significantly smaller as you don't need to pass over the entire data set at each iteration. Furthermore, it's less likely that you will get stuck in local minima while running SGD because a point in the parameter space that is a local minima for the entire data set combined is much less likely to be a local minima for each data point individually. Finally, SGD lends itself to being used for training online models (meaning models built on data points that are arriving at regular intervals) as the entirety of the data does not need to be present in order to train.

\subsection{Backpropagation}
While we've explained at a high level how we are going to use gradient descent to find a good setting for our weight parameters $\textbf{w}$, we have not yet discussed how we are actually going to compute the gradient of our error function with respect to our error function. Thinking again about how our feed forward neural network works, by propagating activations through our network to produce a final output, it's not immediately clear how we can compute gradients for the weights that lie in the middle of our network, as we don't actually have expected results for those intermediate values. The solution to this problem is to send error backwards through our network in a process known as \textit{backpropagation}.

\begin{definition}{Backpropagation}{backpropagation}
Backpropagation is the procedure by which we pass errors backwards through a feed forward neural network in order to compute gradients for the weight parameters of the network.
\end{definition}

Backpropagation refers specifically to the portion of neural network training during which we compute the derivative of the error function with respect to the weight parameters. This is done by propagating errors backwards through the network, hence the name.

\readernote{Backpropagation computes derivatives of the error function with respect to the weight parameters, but we still need to actually update the value of the weight parameters after computing their derivatives. This is typically done using gradient descent or some variant of it.}

We now explore the details of backpropagation in greater depth.

\subsubsection{Computing Derivatives Using Backpropagation}
Recall that the activation $a_{j}$ for an arbitrary node in a neural network can be described by the equation:
\begin{equation} \label{activations-reminder}
	a_{j} = \sum_{m=1}^{M} w_{jm} z_{m}
\end{equation}
where there are $M$ nodes $z_{1}, ..., z_{M}$ connected to the node of interest and weighted by $w_{j1}, ..., w_{jM}$. Recall also that this sum is then transformed by an arbitrary activation function $h(\cdot)$ (which is typically nonlinear) to give the activation $z_{j}$:
\begin{equation} \label{transformed-activations-reminder}
	z_{j} = h(a_{j})
\end{equation}
computing these values as we flow through the network constitutes the forward pass through out network.

We now wish to begin the process of computing derivatives of the error function with respect to our weights. For the sake of simplicity, we'll assume that the current setting of our parameters $\textbf{w}$ generates an error of $E$ for a single data point, as though we were performing stochastic gradient descent.

Let's consider how we could compute the derivative of $E$ with respect to the weight $w_{jm}$:
\begin{equation} \label{deriv-E-wrt-wjm}
	\frac{\partial E}{\partial w_{jm}}
\end{equation}
We first need to figure out what the dependence of $E$ is on this single weight $w_{jm}$. This weight contributes to the final result only via its contribution to the activation $a_{j}$. This allows us to use the chain rule to simplify Equation \ref{deriv-E-wrt-wjm} as:
\begin{equation} \label{chain-rule-deriv-E-wrt-wjm}
	\frac{\partial E}{\partial w_{jm}} = \frac{\partial E}{\partial a_{j}} \cdot \frac{\partial a_{j}}{\partial w_{jm}}
\end{equation}
Using Equation \ref{activations-reminder}, we have that:
\begin{equation*}
	\frac{\partial a_{j}}{\partial w_{jm}} = z_{m}
\end{equation*}
We will also introduce the following notation
\begin{equation} \label{delta-expression}
	\delta_{j} = \frac{\partial E}{\partial a_{j}}
\end{equation}
where the values of $\delta_{j}$ are referred to as \textit{errors}. Now, we are able to rewrite Equation \ref{chain-rule-deriv-E-wrt-wjm} as:
\begin{equation} \label{simplified-chain-rule-deriv-E-wrt-wjm}
	\frac{\partial E}{\partial w_{jm}} = \delta_{j} z_{m}
\end{equation}
The implications of Equation \ref{simplified-chain-rule-deriv-E-wrt-wjm} are significant for understanding backpropagation. This means that the derivative of the error function with respect to an arbitrary weight in the network is equal to the product of the error $\delta$ at the output end of that weight and the value $z$ at the input end of the weight. Thus, to compute all the derivatives of the network, we need only to compute the values of $\delta_{j}$ for each of the nodes in the network. The values of $z_{m}$ should be saved during the forward pass through the network to be multiplied by the values of $\delta_{j}$.
\readernote{We will only have errors $\delta_{j}$ for the hidden and output units of our network. This is logical because there is no notion of applying an error to our input data, which we have no control over.}
We now need only to consider how we should compute the values of $\delta_{j}$, given by Equation \ref{delta-expression}.
For the final layer, also called the output layer, the error is simply the difference between what we computed and the actual result:
\begin{equation*}
	\delta_{j} = y_{k} - \hat{y}_{k}
\end{equation*}
We've seen this term, the difference between the expected and actual results, arise in the gradient of our error expressions for both linear regression and classification in the previous two chapters.

To compute the errors $\delta_{j}$ for the hidden units, we again make use of the chain rule to decompose those errors as follows:
\begin{equation} \label{backprop-for-deltas}
	\delta_{j} = \frac{\partial E}{\partial a_{j}} = \sum_{m=1}^{M} \frac{\partial E}{\partial a_{m}} \frac{\partial a_{m}}{\partial a_{j}} 
\end{equation}
where the summation runs over all of the $M$ nodes to which the node $j$ sends connections. We can rewrite this expression using Equations \ref{delta-expression}, \ref{activations-reminder}, and \ref{transformed-activations-reminder}:
\begin{equation} \label{backprop-formula}
	\delta_{j} = h'(a_{j}) \sum_{m=1}^{M} w_{mj} \delta_{m}
\end{equation}
which means that the value of $\delta$ for a given node can be computed by passing back (backpropagating) the errors $\delta$ from nodes farther up in the network!

Since we know the values of $\delta$ for the final layer of output node, we can recursively apply Equation \ref{backprop-formula} to compute the values of $\delta$ for all the nodes in the network.

Remember that all of these calculations were done for a single data point that generated the error $E$. If we were doing some form of batch gradient descent over multiple data points, we would perform the same calculation for each data point in the batch, and then sum the gradients as follows:

\begin{equation} \label{batch-errors-backprop}
	\frac{\partial E}{\partial w_{jm}} = \sum_{n=1}^{N} \frac{\partial E_{n}}{\partial w_{jm}}
\end{equation}

To solidify understanding of the backpropagation algorithm, it will be useful to try a concrete example.

\begin{example}{Backpropagation Example}{backprop-example}
	TODO...
\end{example}