\chapter{Dimensionality Reduction}
In previous chapters covering supervised learning techniques, we often used basis functions to project our data into higher dimensions prior to applying an inference technique. This allowed us to construct more expressive models, which ultimately produced better results. While it may seem counterintuitive, in this chapter we're going to focus on doing exactly the opposite: reducing the dimensionality of our data. We will primarily consider a dimensionality reduction technique known as Principle Component Analysis (PCA), as well as the reasons behind why we may wish to reduce the dimensionality of our data.

TODO: talk about whether or not this is a supervised technique? talk about variational autoencoders?

\section{Motivation}
Real-world data is often very high dimensional. It's also common that our data sets contain features we are unfamiliar with, either because we are not a domain expert or because the dimensionality is too large for us to comb through the input features by hand.

In these situations, it can be very difficult to manipulate or utilize our data effectively. We don't have a sense for which features are `important' and which ones are just noise. Fitting a model to the data may be computationally prohibitive, and even if we were to fit some sort of model to our data, it may be difficult to interpret why we obtain specific results. It's also hard to gain intuition about our data through visualization since we struggle to think in terms of more than three dimensions. All of these are good reasons that we may wish to reduce the dimensionality of a data set.

\begin{mlcube}{Dimensionality Reduction}
Dimensionality reduction operates primarily on continuous feature spaces, is fully unsupervised, and is non-probabilistic for the techniques we explore in this chapter.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous & Unsupervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

While dimensionality reduction is considered an unsupervised technique, it might be better thought of as a tool used to make data more manageable prior to taking some other action. In fact, it's an important preprocessing step for a host of use cases.

\section{Applications}
As described above, we need a tool like dimensionality reduction in situations where high-dimensional data hinders us. Here are a few specific situations where we would use such a technique:

\begin{enumerate}
    \item Presenting differences between complex molecules in two dimensions (a graph).
    \item Making it feasible to explain the outputs of a payback reliability algorithm.
    \item Efficiently training a neural network to predict supermarket sales on a data set with many input features.
    \item Identifying which costly measurements are worth collecting when experimenting with new chemicals.
\end{enumerate}

With a few of these use cases in mind, we now turn to the mathematical and statistical work that underpins the dimensionality reduction technique known as Priniciple Component Analysis.

\section{Principle Component Analysis}
The main idea behind Principle Component Analysis (PCA) is that we can linearly project our data set onto a subspace without losing too much information. For a simple example, four-dimensional data might primarily live in a subspace that is actually a two-dimensional plane.

One way to think about this is to identify and preserve the features along which there is the most variance. For example, imagine we had a data set comprised of the heights and weights of individual bears. As an extreme case, let's suppose all the bears were exactly the same height but had a wide range of weights.

todo, put that graph of the bears here

To differentiate our data points, we obviously only need to keep the weights of the bears. The variance of the heights is 0, and the variance of the weights is some non-zero number. Intuitively, the most interesting features from our data sets are those that vary the most.

\readernote{In this simple example, the direction of maximal variance occurs exactly along the $x_1$ axis, but in general it will occur on an axis described by a combination of our input features.}

The second way to think about PCA is that we are minimizing the error we incur when we move from the lower-dimensional representation back to the original representation. This is known as \textit{reconstruction loss}. We can consider the meaning of this using our bear example.

Let's say we project the data set from Figure TODO ref figure here down to a single dimension by recording only the weights:

TODO put line graph of weights here

Then, to reconstruct our original graph, we need only to keep track of a slope and bias term in the form of the familiar equation $x_2 = mx_1 + b$. In this case our slope is $m=0$ and our bias $b=8$. Thus we can go from our low-dimensional representation back to our original data:

TODO image of going from one to the other

It will be our goal to determine a low-dimensional representation of our data that allows us to return to our high-dimensional data while losing as little information as possible. The motivation for this is preserving everything salient about the data points while discarding as much unnecessary noise as possible. We now turn to how this can be achieved.

TODO: set the final road map of introductory motivation, moving to the derivation via reconstruction loss that shows we need evecs of cov, discussing the high-level cov interp, now that we're there how do we use linalg to our advantage, then finally what are we actually going to do with PCA, and finally the example. then execute.

\subsection{Reconstruction Loss}
We identified a key idea behind dimensionality reduction in the previous section: we want to find subspaces of our data that preserve as much information as possible. Concretely, this means we want to convert our original data point $\textbf{x}_n$ in $p$ dimensions into a data point $\textbf{x}'_{n}$ in $q$ dimensions for $q < p$. 

\readernote{We're going to assume that our data set has been centered such that each feature in $\textbf{x}_{n}$ has mean 0. This will not affect our results (we can convert back to the uncentered data by adding back the mean of each feature), but will make our derivations substantially easier to work with.}

Let's consider a simple case first: $q=1$. This means that we're projecting our $p$ dimensional data down onto just a single dimension, or in geometric terms, we're projecting our data points $\textbf{x}_{n}$ onto a line through the origin. We can define this line as the unit vector $\textbf{w} \in \mathbb{R}^{p \times 1}$, and the projection is given by the dot product $\textbf{x} \cdot \textbf{w}$.

\readernote{The unit vector $\textbf{w}$ onto which we project our data is known as a \textit{principle component}, from which PCA gets its name.}

This projection produces a scalar, and that scalar defines how far our projection $\textbf{x} \cdot \textbf{w}$ is from the origin. We can convert this scalar to $p$ dimensional space by multiplying it with the unit vector $\textbf{w}$. This means that $(\textbf{x} \cdot \textbf{w})\textbf{w}$ is the result of projecting our data point $\textbf{x}$ down into one-dimension and then converting it to its coordinate location in $p$ dimensions. We refer to these as our \textit{projection vectors}, and we can observe what this looks like geometrically in Figure TODO ref figure here.

TODO: offer figure of what is happening here

The projection vectors we recover from the expression $(\textbf{x} \cdot \textbf{w})\textbf{w}$ will be in $p$ dimensions, but they will obviously not be identical to the original $p$ dimensional vectors (Figure TODO ref figure here demonstrates why that is the case). This difference between the original and projection vectors can be thought of as error, since it is information lost from our original data. For a given data point $\textbf{x}_{n}$ and unit vector $\textbf{w}$, we can measure this error through the expression:
\begin{equation} \label{reconstruction-loss}
	||\textbf{x}_{n} - (\textbf{x} \cdot \textbf{w})\textbf{w}||^{2}
\end{equation}
which is known as \textbf{reconstruction loss} because it measures the error incurred when reconstructing our original data from its projection. TODO: make this a definition?

Reconstruction loss is then a metric for evaluating how `good' a subspace in $q$ dimensions is at representing our original data in $p$ dimensions. The better it is, the less information we lose, and the lower the reconstruction loss becomes.

\subsection{Minimizing Reconstruction Loss}
We now know that our goal is to find a good subspace to project onto, and we also know that finding this good subspace is equivalent to minimizing the reconstruction loss it incurs. Therefore, we now turn to minimizing the reconstruction loss.

First, we can simplify the reconstruction loss for a single data point $\textbf{x}_n$ as follows:
\begin{align*}
	||\textbf{x}_{n} - (\textbf{x} \cdot \textbf{w})\textbf{w}||^{2} &= (\textbf{x}_{n} - (\textbf{x} \cdot \textbf{w})\textbf{w})(\textbf{x}_{n} - (\textbf{x} \cdot \textbf{w})\textbf{w}) \\
	&= ||\textbf{x}_{n}||^{2} - 2(\textbf{x} \cdot \textbf{w})^{2} + (\textbf{x} \cdot \textbf{w})^{2}||\textbf{w}||^{2} \\
	&= ||\textbf{x}_{n}||^{2} - (\textbf{x} \cdot \textbf{w})^{2} \\
\end{align*}
where $||\textbf{w}||^{2} = 0$ as they are unit vectors.

TODO: pickup here, show what it is we are minimizing now with respect to w


TODO: explain that more than one dimension will make sense:
In general, we can project not just onto a line but onto any $q$ dimensional subspace for $q < p$. In this case, our projection operator $\textbf{w}$ will instead be a $p \times q$ dimensional projection matrix that we will denote $\textbf{W}$, and our projection vectors will be given by $(\textbf{W}^{T}\textbf{x})\textbf{W}$.

... do the rest of the derivation here, lead us to the idea of having the covariance matrix, mention thing about cov matrix being safely pos semi-def, mention that once we've identified our subspace, any spanning basis will do. finish with the idea that identifying the directions of maximum variance in our data is indeed what we care about, which we've just proved through reconstruction loss

probably want to break this up into a few more subsections as well

maybe will want to use some of this:

Recall that we can write the covariance of an $m$-dimensional data set $\textbf{X}$ as:

\begin{align*}
	\textbf{S} = cov(\textbf{X}) = \sum_{n=1}^{N} \textbf{x}_{i} \textbf{x}_{i}^{T} = \textbf{X}\textbf{X}^{T}
\end{align*}

\readernote{Notice that we describe the covariance of a data set with the term $\textbf{S}$ instead of the usual covariance term $\Sigma$. This is only convention and has no real significance.}

Perhaps intuitively, the direction of maximum variance in a matrix is given by the eigenvector with the largest eigenvalue. After that, the second largest eigenvalue corresponds to the direction of maximal variance orthogonal to the original eigenvector, and so on. This makes sense given that eigenvectors correspond to the subspaces along which matrices stretch inputs on which they operate.

\subsection{Identifying Directions of Maximal Variance in our Data}
We now know from the previous section that PCA is performed by identifying the directions of maximum variance in our data set, which are going to be conveniently given by the eigenvectors of our covariance matrix. We can now leverage tools from linear algebra to efficiently identify the eigenvectors of a matrix.

... now say how we're going to actually exploit this to get those directions and that subspace: SVD

\subsection{How do We Actually Use PCA}
Now that we have both the derivation, and the high level understanding of its interpretation in the form of the covariance matrix and SVD, what are we going to do from here? Talk about how we actually decide how many dimensions we wish to reduce to, the fact that once we have our subspace, any spanning set of vectors will do, discuss different options for going down to different numbers of dimensions

\subsection{PCA Example}
Run through an example of PCA
