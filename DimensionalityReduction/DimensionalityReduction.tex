\chapter{Dimensionality Reduction}
In previous chapters on supervised learning, we often used basis functions to project our data into higher dimensions prior to applying an inference technique. This allowed us to construct more expressive models, which ultimately produced better results. While it may seem counterintuitive, in this chapter we're going to focus on doing exactly the opposite: reducing the dimensionality of our data. We will primarily consider a dimensionality reduction technique known as Principle Component Analysis (PCA), as well as the reasons behind why we may wish to reduce the dimensionality of our data.

TODO: talk about whether or not this is a supervised technique? talk about variational autoencoders?

\subsection{Motivation}
Real-world data sets often contain data with extremely high dimensionality. It's also common that our data sets contains features we are unfamiliar with, either because we are not a domain expert or because the dimensionality is too large for us to comb through the input features by hand.

In these situations, it can be very difficult to manipulate or utilize our data set effectively. The data is hard to visualize because the dimensionality is so high. We don't have a sense for which features are `important' and which ones are just noise. Fitting a model to the data may be computationally prohibitive, and even if we were to fit some sort of model to our data, it may be difficult to interpret why we obtain specific results. All of these are good reasons that we may wish to reduce the dimensionality of a data set.

\begin{mlcube}{Dimensionality Reduction}
Dimensionality reduction operates primarily on continuous feature spaces, is fully unsupervised, and is non-probabilistic for the techniques we explore in this chapter.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous & Unsupervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

While dimensionality reduction is considered an unsupervised technique, it might be better thought of as a tool used to make data more manageable prior to taking some other action. In fact, it's an important preprocessing step for a host of useful applications.

\subsection{Applications}
As described above, we need a tool like dimensionality reduction in situations where high-dimensional data hinders us. Here are a few specific situations where we would want to use such a technique:

\begin{enumerate}
    \item Presenting differences between complex molecules on a 2D graph.
    \item Making it feasible to explain the outputs of a payback reliability algorithm.
    \item Efficiently training a neural network to predict supermarket sales on a data set with extremely large input features.
    \item Identifying which costly measurements are worth collecting when experimenting with new chemicals.
\end{enumerate}

We now turn to the mathematical and statistical ideas underpinning the dimensionality reduction technique known as Priniciple Component Analysis.

\subsection{Principle Component Analysis}