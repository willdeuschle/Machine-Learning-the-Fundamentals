\chapter{Dimensionality Reduction}
In previous chapters covering supervised learning techniques, we often used basis functions to project our data into higher dimensions prior to applying an inference technique. This allowed us to construct more expressive models, which ultimately produced better results. While it may seem counterintuitive, in this chapter we're going to focus on doing exactly the opposite: reducing the dimensionality of our data. We will primarily consider a dimensionality reduction technique known as Principle Component Analysis (PCA), as well as the reasons behind why we may wish to reduce the dimensionality of our data.

TODO: talk about whether or not this is a supervised technique? talk about variational autoencoders?

\section{Motivation}
Real-world data is often very high dimensional. It's also common that our data sets contain features we are unfamiliar with, either because we are not a domain expert or because the dimensionality is too large for us to comb through the input features by hand.

In these situations, it can be very difficult to manipulate or utilize our data effectively. We don't have a sense for which features are `important' and which ones are just noise. Fitting a model to the data may be computationally prohibitive, and even if we were to fit some sort of model to our data, it may be difficult to interpret why we obtain specific results. It's also hard to gain intuition about our data through visualization since we struggle to think in terms of more than three dimensions. All of these are good reasons that we may wish to reduce the dimensionality of a data set.

\begin{mlcube}{Dimensionality Reduction}
Dimensionality reduction operates primarily on continuous feature spaces, is fully unsupervised, and is non-probabilistic for the techniques we explore in this chapter.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous & Unsupervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

While dimensionality reduction is considered an unsupervised technique, it might be better thought of as a tool used to make data more manageable prior to taking some other action. In fact, it's an important preprocessing step for a host of use cases.

\section{Applications}
As described above, we need a tool like dimensionality reduction in situations where high-dimensional data hinders us. Here are a few specific situations where we would use such a technique:

\begin{enumerate}
    \item Presenting differences between complex molecules in two dimensions (a graph).
    \item Making it feasible to explain the outputs of a payback reliability algorithm.
    \item Efficiently training a neural network to predict supermarket sales on a data set with many input features.
    \item Identifying which costly measurements are worth collecting when experimenting with new chemicals.
\end{enumerate}

With a few of these use cases in mind, we now turn to the mathematical and statistical work that underpins the dimensionality reduction technique known as Priniciple Component Analysis.

\section{Principle Component Analysis}
The main idea behind Principle Component Analysis (PCA) is that we can linearly project our data set onto a subspace without losing too much information. For a simple example, four-dimensional data might primarily live in a subspace that is actually a two-dimensional plane.

One way to think about this is to identify and preserve the features along which there is the most variance. For example, imagine we had a data set comprised of the heights and weights of individual bears. As an extreme case, let's suppose all the bears were exactly the same height but had a wide range of weights.

todo, put that graph of the bears here

To differentiate our data points, we obviously only need to keep the weights of the bears. The variance of the heights is 0, and the variance of the weights is some non-zero number. Intuitively, the most interesting features from our data sets are those that vary the most.

\readernote{In this simple example, the direction of maximal variance occurs exactly along the $x_1$ axis, but in general it will occur on an axis described by a combination of our input features.}

The second way to think about PCA is that we are minimizing the error we incur when we move from the lower-dimensional representation back to the original representation. This is known as \textit{reconstruction loss}. We can consider the meaning of this using our bear example.

Let's say we project the data set from Figure TODO ref figure here down to a single dimension by recording only the weights:

TODO put line graph of weights here

Then, to reconstruct our original graph, we need only to keep track of a slope and bias term in the form of the familiar equation $x_2 = mx_1 + b$. In this case our slope is $m=0$ and our bias $b=8$. Thus we can go from our low-dimensional representation back to our original data:

TODO image of going from one to the other

It will be our goal to determine a low-dimensional representation of our data that allows us to return to our high-dimensional data while losing as little information as possible. The motivation for this is preserving everything salient about the data points while discarding as much unnecessary noise as possible. We now turn to how this can be achieved.

TODO: set the final road map of introductory motivation, moving to the derivation via reconstruction loss that shows we need evecs of cov, discussing the high-level cov interp, now that we're there how do we use linalg to our advantage, then finally what are we actually going to do with PCA, and finally the example. then execute.

\subsection{Reconstruction Loss}
We identified a key idea behind dimensionality reduction in the previous section: we want to find subspaces of our data that preserve as much information as possible. Concretely, this means we want to convert our original data point $\textbf{x}_n$ in $D$ dimensions into a data point $\textbf{x}'_{n}$ in $D'$ dimensions for $D' < D$. 

\readernote{We're going to assume that our data set has been centered such that each feature in $\textbf{x}_{n}$ has mean 0. This will not affect our results (we can convert back to the uncentered data by adding back the mean of each feature), but will make our derivations substantially easier to work with.}

Let's consider a simple case first: $D'=1$. This means that we're projecting our $D$ dimensional data down onto just a single dimension, or in geometric terms, we're projecting our data points $\textbf{x}_{n}$ onto a line through the origin. We can define this line as the unit vector $\textbf{w} \in \mathbb{R}^{D \times 1}$, and the projection is given by the dot product $\textbf{x} \cdot \textbf{w}$.

\readernote{The unit vector $\textbf{w}$ onto which we project our data is known as a \textit{principle component}, from which PCA gets its name.}

This projection produces a scalar, and that scalar defines how far our projection $\textbf{x} \cdot \textbf{w}$ is from the origin. We can convert this scalar to $D$ dimensional space by multiplying it with the unit vector $\textbf{w}$. This means that $(\textbf{x} \cdot \textbf{w})\textbf{w}$ is the result of projecting our data point $\textbf{x}$ down into one-dimension and then converting it to its coordinate location in $D$ dimensions. We refer to these as our \textit{projection vectors}, and we can observe what this looks like geometrically in Figure TODO ref figure here.

TODO: offer figure of what is happening here

The projection vectors we recover from the expression $(\textbf{x} \cdot \textbf{w})\textbf{w}$ will be in $D$ dimensions, but they will obviously not be identical to the original $D$ dimensional vectors (Figure TODO ref figure here demonstrates why that is the case). This difference between the original and projection vectors can be thought of as error, since it is information lost from our original data. For a given data point $\textbf{x}_{n}$ and unit vector $\textbf{w}$, we can measure this error through the expression:
\begin{equation} \label{reconstruction-loss}
	||\textbf{x}_{n} - (\textbf{x} \cdot \textbf{w})\textbf{w}||^{2}
\end{equation}
which is known as \textbf{reconstruction loss} because it measures the error incurred when reconstructing our original data from its projection. TODO: make this a definition?

Reconstruction loss is then a metric for evaluating how `good' a subspace in $D'$ dimensions is at representing our original data in $D$ dimensions. The better it is, the less information we lose, and the lower the reconstruction loss becomes.

\subsection{Minimizing Reconstruction Loss}
We now know that our goal is to find a good subspace to project onto, and we also know that finding this good subspace is equivalent to minimizing the reconstruction loss it incurs. Therefore, we now turn to this optimization problem.

First, we can simplify the reconstruction loss for a single data point $\textbf{x}_n$ as follows:
\begin{align*}
	||\textbf{x}_{n} - (\textbf{x}_{n} \cdot \textbf{w})\textbf{w}||^{2} &= (\textbf{x}_{n} - (\textbf{x}_{n} \cdot \textbf{w})\textbf{w})(\textbf{x}_{n} - (\textbf{x}_{n} \cdot \textbf{w})\textbf{w}) \\
	&= ||\textbf{x}_{n}||^{2} - 2(\textbf{x}_{n} \cdot \textbf{w})^{2} + (\textbf{x}_{n} \cdot \textbf{w})^{2}||\textbf{w}||^{2} \\
	&= ||\textbf{x}_{n}||^{2} - (\textbf{x}_{n} \cdot \textbf{w})^{2} \\
\end{align*}
where $||\textbf{w}||^{2} = 0$ as it is a unit vector. Note that we can define reconstruction loss over our entire data set as follows:
\begin{equation} \label{full-reconstruction-loss}
    RL(\textbf{w}) = \frac{1}{n} \sum_{n=1}^{N} ||\textbf{x}_{n}||^{2} - (\textbf{x}_{n} \cdot \textbf{w})^{2} \\
\end{equation}
Recall that our goal is to minimize reconstruction loss over our data set by optimizing the subspace defined by the vector $\textbf{w}$. Let's first rewrite Equation \ref{full-reconstruction-loss} as:
\begin{align*}
    RL(\textbf{w}) = \frac{1}{n} \sum_{n=1}^{N} ||\textbf{x}_{n}||^{2} - \frac{1}{n} \sum_{n=1}^{N} (\textbf{x}_{n} \cdot \textbf{w})^{2} \\
\end{align*}
where you can now see that our optimization will depend only on maximizing the second term:
\begin{equation} \label{max-for-recon-loss}
    \frac{1}{n} \sum_{n=1}^{N} (\textbf{x}_{n} \cdot \textbf{w})^{2}
\end{equation}
since it is the only one involving \textbf{w}. Recall that the sample mean of a data set is given by the expression $\frac{1}{n} \sum_{n=1}^{N} \textbf{x}_{n}$. Note that Equation \ref{max-for-recon-loss} is the sample mean of $(\textbf{x} \cdot \textbf{w})^{2}$. Recalling the definition of variance:
\begin{align*}
    Var(\textbf{Z}) &= \mathbb{E}(\textbf{Z}^{2}) - (\mathbb{E}(\textbf{Z}))^{2} \\
    \mathbb{E}(\textbf{Z}^{2}) &= Var(\textbf{Z}) + (\mathbb{E}(\textbf{Z}))^{2} \\
\end{align*}
Substituting our terms involving $\textbf{x}_{n} \cdot \textbf{w}$:
\begin{align*}
    \frac{1}{n} \sum_{n=1}^{N} (\textbf{x}_{n} \cdot \textbf{w})^{2} &= Var\big[\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}\big] + \big( \mathbb{E} \big[\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}\big] \big)^{2}
\end{align*}
Recall that we centered our data $\textbf{x}_{n}$ to have mean 0 such that the expression above simplifies to:
\begin{equation} \label{variance-equivalence}
    \frac{1}{n} \sum_{n=1}^{N} (\textbf{x}_{n} \cdot \textbf{w})^{2} = Var\big[\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}\big] \\
\end{equation}
where $Var\big[\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}\big]$ is equivalent to the term we wish to maximize. \textbf{This means that minimizing the reconstruction loss is equivalent to maximizing the variance of our projections $\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}$}.

\readernote{This is an intuitive result. We should like to find a subspace that maintains the spread in our data.}

\subsection{Multiple Principle Components}
Up until now, we've been considering how we would project onto a single principle component $\textbf{w} \in \mathbb{R}^{D \times 1}$. This will reduce our data down to one dimension, just a scalar. In general, we will wish to preserve more of our data than just a single dimension, which means that we will need to have multiple orthogonal principal components. We can then describe the projection of our data $\textbf{x}_{n}$ onto this subspace as the sum of the projections onto $D'$ orthogonal vectors:
\begin{equation} \label{orthogonal-projections}
    \sum_{d'=1}^{D'} (\textbf{x}_{n} \cdot \textbf{w}_{d'})\textbf{w}_{d'}
\end{equation}

\subsection{Identifying Directions of Maximal Variance in our Data}
We now know from the previous section that we find our principle components (and thus the subspace we will project onto) by identifying the directions of maximum variance in our projected data set. We know from Equation \ref{variance-equivalence} that the variance is equivalent to:
\begin{equation*}
    \sigma^{2}_{\textbf{w}} \equiv Var\big[\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}\big] = \frac{1}{n} \sum_{n=1}^{N} (\textbf{x}_{n} \cdot \textbf{w})^{2}
\end{equation*}
Rewriting this in terms of matrix notation we have that:
\begin{equation*}
    \sigma^{2}_{\textbf{w}} = \frac{1}{n} (\textbf{X} \textbf{w})^{T} (\textbf{X} \textbf{w})
\end{equation*}
We can further simplify this:
\begin{align*}
    \sigma^{2}_{\textbf{w}} &= \frac{1}{n} \textbf{w}^{T}\textbf{X}^{T} \textbf{X} \textbf{w} \\
    \sigma^{2}_{\textbf{w}} &= \frac{1}{n} \textbf{w}^{T}\textbf{X}^{T} \textbf{X} \textbf{w} \\
    \sigma^{2}_{\textbf{w}} &= \textbf{w}^{T} \frac{\textbf{X}^{T}\textbf{X}}{n} \textbf{w} \\
    \sigma^{2}_{\textbf{w}} &= \textbf{w}^{T} \textbf{S} \textbf{w} \\
\end{align*}
where $\textbf{S} = \frac{\textbf{X}^{T}\textbf{X}}{n}$ is the empirical covariance matrix of our data set.

\readernote{Notice that by convention we describe the empirical covariance of a data set with the term $\textbf{S}$ instead of the usual covariance term $\Sigma$.}

Our goal is to maximize the term $\sigma^{2}_{\textbf{w}} = Var\big[\{\textbf{x}_{n} \cdot \textbf{w}\}_{n=1}^{N}\big]$ with respect to $\textbf{w}$. Furthermore, $\textbf{w}$ is a unit vector, so we must optimize subject to the constraint $\textbf{w}^{T}\textbf{w} = 1$. Recalling the discussion of Lagrange multipliers from Chapter 6 on Support Vector Machines, we incorporate this constraint by reformulating our optimization problem as the Lagrangian equation:
\begin{align*}
    \mathcal{L}(\textbf{w}, \lambda) &= \textbf{w}^{T} \textbf{S} \textbf{w} - \lambda(\textbf{w}^{T}\textbf{w} - 1) \\
\end{align*}
As usual, we proceed by taking the derivative with respect to each parameter:
\begin{align*}
    \frac{d\mathcal{L}(\textbf{w}, \lambda)}{d\textbf{w}} &= 2\textbf{S} \textbf{w} - 2\lambda\textbf{w} \\
    \frac{d\mathcal{L}(\textbf{w}, \lambda)}{d\lambda} &= \textbf{w}^{T}\textbf{w} - 1 \\
\end{align*}
We can now set these equal to 0 and solve for the optimal values:
\begin{align*}
    \textbf{S} \textbf{w} &= \lambda\textbf{w} \\
    \textbf{w}^{T}\textbf{w} &= 1 \\
\end{align*}
This result is very significant! As we knew already, we needed $\textbf{w}$ to be a unit vector. However, we also see that $\textbf{w}$ is an eigenvector of the empirical covariance matrix $\textbf{w}$. Futhermore, the eigenvector that will maximize our quantity of interest $\sigma^{2}_{\textbf{w}} = \textbf{w}^{T} \textbf{S} \textbf{w}$ will be the eigenvector with the largest eigenvalue $\lambda$. Fortunately, linear algebra gives us many tools for finding eigenvectors, and as a result we can efficiently identify our principal components.

To recap, we've learned that the optimal principal components (meaning the vectors describing our projection subspace) are the eigenvectors of the empirical covariance matrix of our data set. The vector preserving the most variance in our data (and thus minimizing the reconstruction loss) is given by the eigenvector with the largest eigenvalue, followed by the eigenvector with the next largest eigenvalue, and so on. Furthermore, while it is somewhat outside the scope of this textbook, we are guaranteed to have $D$ distinct, orthogonal eigenvectors with eigenvalues $\geq 0$. This is a result of linear algebra that hinges on the fact that our empirical covariance matrix $\textbf{S}$ is symmetric and positive semi-definite.

\subsection{Choosing the Optimal Number of Principal Components}
We now know that we can find the principal components that will compose our projection subspace by identifying the eigenvectors of the empirical covariance matrix $\textbf{S}$. Note that the exact procedure for finding these eigenvectors is a topic better suited for a book on linear algebra, but if you are interested, you can look into a topic known as Singular Value Decomposition (SVD). For our purposes, we will assume that we have a black box to which we can pass $\textbf{S}$ and get back a list of the $D$ principal components.

Because these principal components are orthogonal, the projections they will produce will be entirely uncorrelated. This means we can sum the projections of our original data onto each component individually to create our lower dimensional data points. Additionally, the orthogonality of these $D$ principal components means that they span the entire $D$ dimensional space of our original data set. That being said, it doesn't make sense that we would use every principal component to define our projection subspace, since that wouldn't lead to a reduction in the dimensionality of our data at all. We now need to how many principal components we will choose to include, and therefore what subspace we will be projecting onto.

How many principal components we choose to include depends on our goals. For example, if we simply wish to visualize our data, then we would like to project onto a 2D or 3D surface (which is possible for humans to visualize). Therefore, we would choose the first 2 or 3 principal components, and project our original data onto the subspace defined by those vectors. This might look something like Figure TODO REF FIGURE HERE:

TODO create figure where we use PCA and visualize our data.

However, it's more complicated to choose the optimal number of principal components when our goal is not to visualize the data. We're now left with the task of trading off how much dimensionality reduction we wish to achieve with how much information we want to preserve from our data. Depending on your use case, you may have a hard cutoff for how much data you can retain, in which case you should choose that number of principal components. However, it's more common that you will simply want to keep around as many principal components as proves to be useful.

One way to do this is to refer back to the informal `elbow' method described for K-Means clustering. In this case, we can graph our reconstruction loss against the number of principal components we use, as seen in Figure TODO ref figure here. The idea here is to add principal components to your subspace one at a time, calculating the reconstruction loss as you go. The first few principal components will greatly reduce the reconstruction loss, before eventually leveling off. You can identify the `elbow' where the reduction in loss starts to diminish, and choose to use that number of principal components.

TODO: make figure of reconstruction loss vs. number of principal components

Another way to do this is to consider how much variance you wish to preserve in your data. Each principal component is associated with an eigenvalue $\lambda_{d}$ that indicates what proportion of the variance that principal component is responsible for in your data set. Then the fraction of variance retained from your data set if you choose to keep $D'$ principal components is given by:
\begin{equation} \label{variance-retention}
    \text{retained variance} = \frac{\sum_{d'=1}^{D'} \lambda_{d'}}{\sum_{d=1}^{D} \lambda{d}}
\end{equation}
For different applications, there may be different levels of acceptable variance retention, which can help you decide how many principal components to keep.

Finally, we'd like make a comment about what happens after we've selected our principal components. Upon selecting our principal components, we've defined a subspace onto which we will be projecting our original data. And although this subspace is defined by the basis given by our principal components, these principal components are not a unique description of that subspace. We could choose to use any basis once we've identified our subspace through the principal components. The importance of this idea is simply that although our principal components are unique, using them to define our projection subspace is not a unique choice.

\subsection{PCA Example}
Run through an example of PCA here
