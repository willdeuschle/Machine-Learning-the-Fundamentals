\chapter{Support Vector Machines}
In this chapter, we will explore what is known as a support vector machine, or SVM for short. SVMs are broadly useful for problems in classification and regression, and they draw on techniques from what are known as \textit{kernel algorithms}. This simply means they make use of an arbitrary kernel function $k(\cdot)$ to compare data points in our training set to new data points about which we wish to make a prediction. One of the most important aspects of SVMs is that they decompose into convex optimization problems, for which we can find a global optimum with relative ease. We will explore the mathematical underpinnings of SVMs, which can be slightly more challenging than our previous topics, as well as their typical use cases.

\section{Motivation}
While SVMs can be used for classification or regression, we will reason about them in the classification case for simplicity. 

The grand idea behind SVMs is that we should construct a linear hyperplane in our feature space that maximally separates our classes, which means that the different classes should be as far from that hyperplane as possible. The distance of our data from the hyperplane is known as `margin'.

\begin{definition}{Margin}{definition}
Margin is the distance of data points from the separating hyperplane of an SVM model. Larger margins lead to more generalizable models.
\end{definition}

A larger margin tends to mean that our model will generalize better to new data, since greater margin allows more wiggle room to be correct about classifying new incoming data.

TODO: image of a hyperplane in 2d between different classes of points

This idea of margin is quite intuitive for humans. If you were presented with graph (TODO reference graph here) and were asked to separate the two classes, you would likely draw the line that keeps data points as far from it as possible. SVMs and other margin-based methods will attempt to algorithmically recreate this intuition that humans are quite good at.

\subsection{Max Margin Methods}
SVMs are a specific instance of a broader class of model known as max margin methods. The name describes these methods quite well: they deal with creating a maximum margin between training data and some form of a model, with the idea that this represents the model of maximum generalizability. 

Other max margin methods are outside the scope of this textbook. Note that these alternative methods typically differ from SVMs in some non-trivial manner. For example, SVMs do not produce posterior probability distributions but rather decision rules for handling new data points. If you needed posterior probabilities for your model, there are other max-margin methods better suited to that task.

\begin{mlcube}{Support Vector Machines}
SVMs typically operate over inputs that exist in a continuous feature space. We need labeled training data to identify the relevant hyperplane in an SVM model. Finally, SVMs operate in a non-probabilistic setting.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Continuous & Supervised & No \\
    \end{tabular}
\end{center}
\end{mlcube}

\subsection{Applications}
The theory behind SVMs has been around for quite some time (since 1963), so prior to the rise of neural networks and other more computationally intensive techniques, SVMs were used quite extensively for image recognition, categorization, and other typical machine learning tasks.

In particular, SVMs were and still are widely used for a subset of classification problems known as anomaly detection.
\readernote{The purpose of anomaly detection is to identify unusual data points. For example, if we are manufacturing widgets, we may wish to inspect and flag any widget that seems atypical with respect to the rest of the widgets we produce.}
Anomaly detection can be as simple as a binary classification problem where the data set is comprised of anomalous and non-anomalous data points. As we will see, an SVM can be constructed from this data set to identify future anomalous points very efficiently.

\section{Hard Margin Classifier}
We will learn the theory behind SVMs by starting with a simple two-class classification problem, as we've seen several times in previous chapters. We will constrain the problem even further by declaring that the two classes are linearly separable, which is the basis of the hard margin formulation for SVMs.

\subsection{Why the Hard Margin}
The hard margin constraint ensures that our data is linearly separable. We will see that this is actually not a requirement for constructing an SVM, but it simplifies the problem initially and makes our derivations significantly easier. After we've established the hard margin formulation, we will extend the technique to work in situations where our data is not linearly separable.

\subsection{Deriving our Optimization Problem}
Recall that our goal is to define a hyperplane separating our data points that establishes the maximum distance from our training data. To uncover this hyperplane, we start with a simple linear model for a two-class classification problem:
\begin{equation*}
y(\textbf{x}) = \textbf{w}^{T}\phi(\textbf{x}) + w_{0}
\end{equation*}
where we have $N$ multidimensional data points $x_{1}, ..., x_{N}$, $\phi(\cdot)$ is a standard basis transformation function, and there is a bias term $w_{0}$. Each of our data point has a class $y_{1}, ..., y_{N}$ which is either $1$ or $-1$, and we assign new data points to class $1$ or $-1$ according to the sign of our trained model $y(\textbf{x})$.

By specifying our model this way, we have implicity defined a hyperplane separating our two classes given by:
\begin{equation} \label{implicit-hyperplane}
	\textbf{w}^{T}\phi(\textbf{x}) + w_{0} = 0
\end{equation}
This is not an intuitive result, but we can increase clarity through a simple derivation.

\begin{derivation}{Hyperplane Derivation}{hyperplane-derivation}
	Imagine two data points $x_{1}$ and $x_{2}$ on the hyperplane defined by $\textbf{w}^{T}\phi(\textbf{x}) + w_{0} = 0$. When we project their difference onto our model $\textbf{w}$, we find:
	\begin{align*}
		\textbf{w}^{T}(\textbf{x}_{1} - \textbf{x}_{2}) = \textbf{w}^{T}\textbf{x}_{1} - \textbf{w}^{T}\textbf{x}_{2} = -w_{0} - (-w_{0}) = 0
	\end{align*}
	which means that $\textbf{w}$ is orthogonal to our hyperplane.
\end{derivation}
TODO: provide image here of data points, our hyperplane, and the orthogonal \textbf{w}

Remember that we're trying to maximize the margin between our training data and the hyperplane. The fact that $\textbf{w}$ is orthogonal to our hyperplane allows us to determine how far our data points are from the hyperplane.

To determine the distance between a training point $\textbf{x}$ and the hyperplane, which we denote $d$, we need the distance in the direction of $\textbf{w}$ between the point and the hyperplane. We denote $\textbf{x}_{\perp}$ to be the projection of $x$ onto the hyperplane, which allows us to decompose $x$ as the following:
\begin{equation} \label{decompose-x}
	\textbf{x} = \textbf{x}_{\perp} + d \frac{\textbf{w}}{|| \textbf{w} ||}
\end{equation}
which is simply the sum of the portion of $\textbf{x}$ perpendicular to $\textbf{w}$ and the portion of $\textbf{x}$ that is parallel to $\textbf{w}$. From here we can solve for $d$:
\begin{derivation}{Distance from Hyperplane Derivation}{distance-from-hyperplane-derivation}
	We start by left multiplying Equation \ref{decompose-x} with $\textbf{w}^{T}$.
	\begin{align*}
		\textbf{w}^{T}\textbf{x} = \textbf{w}^{T}\textbf{x}_{\perp} + d \frac{\textbf{w}^{T}\textbf{w}}{||\textbf{w}||}
	\end{align*}
	Simplifying:
	\begin{align*}
		\textbf{w}^{T}\textbf{x} =  - w_{0} + d ||\textbf{w}||
	\end{align*}
	Rearranging:
	\begin{align*}
		d = \frac{\textbf{w}^{T}\textbf{x} + w_{0}}{||\textbf{w}||}
	\end{align*}
\end{derivation}
which means that for each data point $\textbf{x}$, we now have the signed distance of that data point from the hyperplane.

When we classify our data correctly, the distance $d$ will be positive for $y_{n} = 1$, and $d$ will be negative for $y_{n} = -1$. For consistency and training purposes, we can make the margin positive whenever we correctly classify data by multiplying $y_{n}$ and $d$. Then, the margin for an individual data point $\textbf{x}_{n}$ is given by:
\begin{equation} \label{individual-margin}
	\frac{y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0})}{||\textbf{w}||}
\end{equation}
The margin for an entire data set is given by the margin to the closest point in the set, given by:
\begin{equation} \label{total-margin}
	\min_{n} \frac{y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0})}{||\textbf{w}||}
\end{equation}
Then, it is our goal to maximize this margin with respect to our model parameters $\textbf{w}$ and $w_{0}$. This is given by:
\begin{equation} \label{total-maximized-margin}
	\arg\max_{\textbf{w}, w_{0}} \big\{ \frac{1}{||\textbf{w}||} \min_{n} y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0}) \big\}
\end{equation}
Note that this is a hard problem to optimize, but we can make it more tractable by recognizing some important features of Equation \ref{total-maximized-margin}. First, rescaling $\textbf{w} \rightarrow \alpha \textbf{w}$ and $w_{0} \rightarrow \beta w_{0}$ has no impact on the relative distance of any data point $\textbf{x}_{n}$ from the hyperplane. We can use this rescaling liberty to enforce
\begin{equation} \label{enfore-dist-to-1}
	y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0}) = 1
\end{equation}
for the data point closest to the hyperplane. Thus, all of our data points have a margin that is greater than or equal to 1:
\begin{equation} \label{new-margin-constraint}
	\forall n \, y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0}) \geq 1
\end{equation}
which is used as a constraint for the optimization problem in Equation \ref{total-maximized-margin}. Thus our optimization problem now looks like:
\begin{equation} \label{simplified-maximized-margin-optimization}
	\arg\max_{\textbf{w}, w_{0}} \frac{1}{||\textbf{w}||} \quad \text{s.t.} \quad \forall n \, y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0}) \geq 1
\end{equation}
Notice also that we are maximizing $\frac{1}{||\textbf{w}||}$, which is equivalent to minimizing $||\textbf{w}||^{2}$. We will also add a constant term $\frac{1}{2}$ for convenience in optimizing later on, to give us our final optimization problem:
\begin{equation} \label{final-simplified-maximized-margin-optimization}
	\arg\min_{\textbf{w}, w_{0}} \frac{1}{2} ||\textbf{w}||^{2} \quad \text{s.t.} \quad \forall n \, y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0}) \geq 1
\end{equation}
Note that Equation \ref{final-simplified-maximized-margin-optimization} is now a quadratic programming problem - meaning we wish to optimize a quadratic function subject to a set of linear constraints on our parameters. We will discuss shortly how we actually optimize this function.

\subsection{What is a Support Vector}
TODO: should this go here? move downward more?
Up until now, we have been discussing Support Vector Machines without actually identifying what a support vector is. We now have enough information from the derivation in the previous section to define them: the support vectors in an SVM are the data points closest to the hyperplane. In the hard margin case we have constrained the closest data points to being a distance of 1 from the hyperplane, so the support vectors are all $d=1$ from the hyperplane.

\readernote{After we have optimized an SVM in the hard margin case, we must have at least two support vectors with a distance of 1 from the hyperplane.}

\subsection{Hard Margin SVM Example}
.. TODO: I think just draw one graph of data points with two classes and then the line that will be separating them?

\section{Soft Margin Classifier}
Thus far, we've been operating under the assumption that our data is linearly separable in transformed feature space, which allowed us some nice guarantees in the derivations of the previous section. For example, given that our data was linearly separable, we could guarantee that every data point would be on the correct side of the hyperplane, which was part of what allowed us to enforce the constraint that $d=1$ for the points closest to the hyperplane. We now seek to generalize the work of the previous section to situations where our data is not so nice.

\subsection{Why the Soft Margin?}
What if our data is not linearly separable in transformed feature space? For a real, complex problem domain, it's highly unlikely that it would be. However, as of now our SVM formulation would be useless with non-linearly separable data. That is why we need the soft margin SVM.

At a high level, the soft margin SVM allows for some of our data points to be closer to or even on the incorrect side of the hyperplane. This is highly necessary if our data set is not linearly separable. It is also quite logical. Examining Figure (TODO: ref figure here), we see that we have a single outlier data point. We can still create a good model by just allowing this single data point to be close to the hyperplane. That is what the soft margin formulation will allow for.
TODO: put that figure ref'd above right here.

\subsection{Updated Optimization Problem for Soft Margins}
To enable the soft margin formulation, we introduce what are known as \textit{slack variables} denoted $\xi_{n} \geq 0$, which simply relax the constraints we imposed in the hard margin formulation. There is a slack variable $\xi_{n} \geq 0$ for every variable $x_{n}$, and they take the following values:
\begin{equation} \label{slack-variable-values}
	\xi_{n} = \begin{cases}
	 	= 0 & \text{if correctly classified} \\
		\in (0, 1] & \text{if correctly classified but inside margin} \\
		> 1 & \text{if incorrectly classified} \\
	\end{cases}
\end{equation}
These slack variable penalize data points within the margin defined by the hyperplane, but they don't forbid us from having data points close to the hyperplane if it produces the best model. We now reformulate our optimization problem as:
\begin{equation} \label{soft-margin-optimization-problem}
	\arg\min_{\textbf{w}, w_{0}} \frac{1}{2} ||\textbf{w}||^{2} + C \sum_{n=1}^{N} \xi_{n} \quad \text{s.t.} \quad \forall n \, y_{n}(\textbf{w}^{T}\textbf{x}_{n} + w_{0}) \geq 1 - \xi_{n}, \quad \xi_{n} \geq 0
\end{equation}
TODO: make the argmax thing look better with the variables acutally under the `argmax' \newline \newline
where $C$ is a regularization parameter that determines how heavily we penalize violations of the hard margin constraints. A large value of $C$ penalizes violation of the hard margin constraints, which means our model will follow the data closely have small regularization. A small value of $C$ won't heavily penalize having data points inside the margin region, relaxing the constraint and allowing our model to somewhat disregard more of the data. This boosts regularization.

\readernote{Unlike most regularization parameters we've seen thus far, $C$ increases regularization as it gets smaller.}

\subsection{Soft Margin Support Vectors}
Under the the hard margin formulation, the support vectors were those data points exactly $d=1$ away from the hyperplane, and they were also guaranteed to be the points closest to the hyperplane. Under the soft margin formulation, we no longer have this guarantee since we explicity relaxed it in the name of creating better, more generalizable models.

The support vectors for the soft margin case are those which fall within the margin region (including the boundary) or those that are misclassified. This corresponds directly to the value of the slack variable $\xi_{n}$ defined in Equation \ref{slack-variable-values}: data points for which $\xi_{n} > 0$ are the support vectors. Note that these support vectors are either correctly classified but within/on the margin ($\xi_{n} \in [0,1]$) or incorrectly classified ($\xi_{n} > 1$). We can visualize this in Figure (TODO: ref figure with the tube here).

\readernote{Data points for which $\xi_{n} = 0$ are on the margin and are still considered support vectors.}

TODO: draw the soft margin tube so people can see what the support vectors are

\subsection{Soft Margin SVM Example}
.. TODO: I think just draw one graph of data points with two classes and then the line that will be separating them but now with data points that have the slack variable also defined?

\section{Conversion to Dual Form}
Now that we understand the formulation of the optimization problem for SVMs, we need to discuss how we actually go about optimizing to produce a model solution. This will involve producing a \textit{dual form} for our problem. We will solve in the case of the hard margin case for notational simplicity, but our solution will apply to the soft margin formulation as well.

\readernote{A dual form is simply an equivalent manner of representing some expression, in this case the quadratic programming problem we need to optimize.}

\subsection{Lagrange Multipliers}
Before we get into deriving the dual formation, we need to be aware of a critical piece of math that will enable us to solve our optimization problem: \textit{Langrange multipliers}.

A Lagrange multiplier is used to find optima of a function subject to certain constraints. This is exactly what we need to do with the optimization problem described by Equation \ref{final-simplified-maximized-margin-optimization}.

The underlying theory behind Lagrange multipliers is not overly difficult to understand, but it is beyond the scope of this textbook. Instead, we will offer the method by which you can use them to solve optimization problems, while leaving the underlying theory untouched.

If you have a function $f(\textbf{x})$ which you need to optimize (let's use the example of maximization here, but minimization applies just the same) subject to the constraint that some function $g(\textbf{x}) = 0$, you can take the following steps. First, construct the Lagrangian function $L(\textbf{x}, \lambda)$:

\begin{equation*}
	L(\textbf{x}, \lambda) = f(\textbf{x}) + \lambda g(\textbf{x})
\end{equation*}

Then, set the derivative of $L$ with respect to both $\textbf{x}$ and $\lambda$ equal to 0:

\begin{equation*}
	\nabla L_{\textbf{x}} = 0, \qquad \frac{\partial L}{\partial \lambda} = g(\textbf{x}) = 0
\end{equation*}

If $\textbf{x}$ is $D$-dimensional, this will give you a system of $D+1$ equations. You can solve these equations for $\textbf{x}$ to find the optimal value of $f(\textbf{x})$ subject to the constraint $g(\textbf{x})$.

The only other gotcha to be aware of is the case where your constraint $g(\textbf{x})$ is an inequality. In this case, construct your Lagrangian function as follows:
\begin{equation*}
	L(\textbf{x}, \lambda) = f(\textbf{x}) - \lambda g(\textbf{x})
\end{equation*}
which you again optimize with respect to the parameters $\textbf{x}$ and $\lambda$:
\begin{equation*}
	\nabla L_{\textbf{x}} = 0, \qquad \frac{\partial L}{\partial \lambda} = g(\textbf{x}) \geq 0
\end{equation*}
but now also subject to the constraints:
\begin{equation*}
	\qquad \lambda \geq 0, \qquad \lambda g(\textbf{x}) = 0
\end{equation*}

\readernote{The exact reason for these new constraints when working with inequalities is beyond the scope of this textbook, but it's important to remember that these two additional constraints be met.}

\subsection{Deriving the Dual Formulation}
\subsection{Why is the Dual Formulation Necessary?}
\subsection{Kernel Composition}
